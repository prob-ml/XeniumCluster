{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from importlib import reload\n",
    "\n",
    "# this ensures that I can update the class without losing my variables in my notebook\n",
    "import xenium_cluster\n",
    "reload(xenium_cluster)\n",
    "from xenium_cluster import XeniumCluster\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your .gz file\n",
    "file_path = 'data/hBreast/transcripts.csv.gz'\n",
    "\n",
    "# Read the gzipped CSV file into a DataFrame\n",
    "df_transcripts = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "# drop cells without ids\n",
    "df_transcripts = df_transcripts[df_transcripts[\"cell_id\"] != -1]\n",
    "\n",
    "# drop blanks and controls\n",
    "df_transcripts = df_transcripts[~df_transcripts[\"feature_name\"].str.startswith('BLANK_') & ~df_transcripts[\"feature_name\"].str.startswith('NegControl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(data, dataset_name: str, current_spot_size: int, third_dim: bool, init_method: str = \"mclust\", num_pcs: int = 15, n_clusters=15):\n",
    "    \n",
    "    clustering = XeniumCluster(data=data, dataset_name=dataset_name)\n",
    "    clustering.set_spot_size(current_spot_size)\n",
    "    clustering.create_spot_data(third_dim=third_dim, save_data=True)\n",
    "\n",
    "    print(f\"The size of the spot data is {clustering.xenium_spot_data.shape}\")\n",
    "\n",
    "    clustering.normalize_counts(clustering.xenium_spot_data)\n",
    "    clustering.generate_neighborhood_graph(clustering.xenium_spot_data, plot_pcas=False)\n",
    "\n",
    "    BayesSpace_cluster = clustering.BayesSpace(clustering.xenium_spot_data, init_method=init_method, num_pcs=num_pcs, K=n_clusters)\n",
    "\n",
    "    return clustering, BayesSpace_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def record_results(original_data, cluster_dict, results_dir, model_name, filename, spot_size, third_dim, num_pcs, init_method, K=None, resolution=None, uses_spatial=True):\n",
    "\n",
    "    dirpath = f\"{results_dir}/{model_name}/{num_pcs}/{(str(resolution) if resolution is not None else str(K))}/clusters/{init_method}/{spot_size}\"\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "    for gamma in np.linspace(1, 3, 9):\n",
    "        gamma_str = f\"{gamma:.2f}\"\n",
    "        try:\n",
    "            current_clustering = pd.read_csv(f\"{dirpath}/{gamma_str}/{filename}.csv\", index_col=0)[\"BayesSpace cluster\"].values\n",
    "\n",
    "            original_data.xenium_spot_data.obs[f\"{model_name} cluster\"] = current_clustering\n",
    "            # Extracting row, col, and cluster values from the dataframe\n",
    "            rows = torch.tensor(original_data.xenium_spot_data.obs[\"row\"].astype(int))\n",
    "            cols = torch.tensor(original_data.xenium_spot_data.obs[\"col\"].astype(int))\n",
    "            clusters = torch.tensor(original_data.xenium_spot_data.obs[f\"{model_name} cluster\"].astype(int))\n",
    "            cluster_labels = np.unique(clusters)\n",
    "            num_clusters = len(cluster_labels)\n",
    "\n",
    "            num_rows = int(max(rows) - min(rows) + 1)\n",
    "            num_cols = int(max(cols) - min(cols) + 1)\n",
    "\n",
    "            cluster_grid = torch.zeros((num_rows, num_cols), dtype=torch.int)\n",
    "\n",
    "            cluster_grid[rows, cols] = torch.tensor(clusters, dtype=torch.int)\n",
    "            \n",
    "            wss = {}\n",
    "            for label in cluster_labels:\n",
    "                current_cluster_locations = torch.stack(torch.where((cluster_grid == label)), axis=1).to(float)\n",
    "                wss[f\"Cluster {label}\"] = (spot_size ** 2) * torch.mean(torch.cdist(current_cluster_locations, current_cluster_locations)).item()\n",
    "                print(f\"POSSIBLE {len(cluster_labels)}\", label, wss[f\"Cluster {label}\"])\n",
    "\n",
    "            wss_dirpath = f\"{results_dir}/{model_name}/{num_pcs}/{(str(resolution) if resolution is not None else str(K))}/wss/{init_method}/{spot_size}\"\n",
    "            if not os.path.exists(wss_dirpath):\n",
    "                os.makedirs(wss_dirpath)\n",
    "\n",
    "            wss_filepath = f\"{wss_dirpath}/{gamma_str}/{filename}_wss.json\"\n",
    "            with open(wss_filepath, \"w\") as f:\n",
    "                json.dump(wss, f, indent=4)\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {\"BayesSpace\": {}}\n",
    "wss = {\"BayesSpace\": {}}\n",
    "results_dir = \"results/hBreast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_list = [3, 5, 10, 15, 25]\n",
    "init_methods = [\"kmeans\", \"mclust\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "for spot_size in [50, 75, 100]:\n",
    "    for third_dim in [False]:\n",
    "        for K in [17]:\n",
    "            for num_pcs in PC_list:\n",
    "                for init_method in init_methods:\n",
    "                    cluster_results_filename = f\"clusters_K={K}\"\n",
    "                    original_data, BayesSpace_cluster = run_experiment(df_transcripts, \"hBreast\", spot_size, third_dim, init_method, num_pcs, n_clusters=K)\n",
    "\n",
    "                    # BayesSpace\n",
    "                    if \"BayesSpace\" not in cluster_dict:\n",
    "                        cluster_dict[\"BayesSpace\"] = {}\n",
    "                    if spot_size not in cluster_dict[\"BayesSpace\"]:\n",
    "                        cluster_dict[\"BayesSpace\"][spot_size] = {}\n",
    "                    if third_dim not in cluster_dict[\"BayesSpace\"][spot_size]:\n",
    "                        cluster_dict[\"BayesSpace\"][spot_size][third_dim] = {}\n",
    "                    if init_method not in cluster_dict[\"BayesSpace\"][spot_size][third_dim]:\n",
    "                        cluster_dict[\"BayesSpace\"][spot_size][third_dim][init_method] = {}\n",
    "                    cluster_dict[\"BayesSpace\"][spot_size][third_dim][init_method][num_pcs] = {True: {K: BayesSpace_cluster.tolist()}}\n",
    "                    record_results(original_data, cluster_dict, results_dir, \"BayesSpace\", cluster_results_filename, spot_size, third_dim, num_pcs, init_method, K, uses_spatial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_sizes = [50, 75, 100]\n",
    "in_billions = 1_000_000_000\n",
    "method=\"BayesSpace\"\n",
    "for spot_size in spot_sizes:\n",
    "    for K in [17]:\n",
    "        for init_method in init_methods:\n",
    "            for num_pcs in PC_list:\n",
    "                for gamma in np.linspace(1, 3, 9):\n",
    "                    gamma_str = f\"{gamma:.2f}\"\n",
    "                    cluster_results_filename = f\"clusters_K={K}\"\n",
    "                    filename = f\"results/hBreast/{method}/{num_pcs}/{K}/wss/{init_method}/{spot_size}/{gamma_str}/{cluster_results_filename}_wss.json\"\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, \"r\") as wss_dict:\n",
    "                            current_wss = json.load(wss_dict)\n",
    "                        print(\"Method:\", method, \"Spot Size\", spot_size, \"Num Clusters:\", len(current_wss), \"Num PCs\", num_pcs, \"\\u03B3\", f\": {gamma_str}\", \"Initial Method:\", init_method, \"Total WSS:\", sum(current_wss.values()) / in_billions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xenium-1YUjn3qu-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
