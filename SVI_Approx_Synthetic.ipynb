{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import pyro\n",
    "import json\n",
    "import math\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import PyroOptim\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.cm import get_cmap\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import KDTree\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from importlib import reload\n",
    "\n",
    "# this ensures that I can update the class without losing my variables in my notebook\n",
    "import xenium_cluster\n",
    "reload(xenium_cluster)\n",
    "from xenium_cluster import XeniumCluster\n",
    "from utils.metrics import *\n",
    "from utils.BayesSpace import *\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"YAY! GPU available :3\")\n",
    "    \n",
    "    # Get all available GPUs sorted by memory usage (lowest first)\n",
    "    available_gpus = GPUtil.getAvailable(order='memory', limit=1)\n",
    "    \n",
    "    if available_gpus:\n",
    "        selected_gpu = available_gpus[0]\n",
    "        \n",
    "        # Set the GPU with the lowest memory usage\n",
    "        torch.cuda.set_device(selected_gpu)\n",
    "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        print(f\"Using GPU: {selected_gpu} with the lowest memory usage.\")\n",
    "    else:\n",
    "        print(\"No GPUs available with low memory usage.\")\n",
    "else:\n",
    "    print(\"No GPU available :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def match_labels(true_soft_assignments, inferred_soft_assignments):\n",
    "    \"\"\"\n",
    "    Match inferred cluster labels to true cluster labels using the Hungarian algorithm.\n",
    "    \n",
    "    Args:\n",
    "        true_soft_assignments (torch.Tensor): True soft cluster assignments (N x K).\n",
    "        inferred_soft_assignments (torch.Tensor): Inferred soft cluster assignments (N x K).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Permutation indices for inferred cluster labels.\n",
    "    \"\"\"\n",
    "    N, K = true_soft_assignments.shape\n",
    "    \n",
    "    # Compute distance matrix between cluster distributions (K x K)\n",
    "    distance_matrix = torch.cdist(true_soft_assignments.T, inferred_soft_assignments.T, p=2)\n",
    "    \n",
    "    # Solve the assignment problem\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix.detach().cpu().numpy())\n",
    "    \n",
    "    # Convert to tensor\n",
    "    col_ind = torch.tensor(col_ind, device=true_soft_assignments.device, dtype=int)\n",
    "    \n",
    "    # Return column permutation indices\n",
    "    return col_ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_synthetic_data(\n",
    "    grid_size = 50,\n",
    "    num_clusters = 5,\n",
    "    data_dimension = 5,\n",
    "    random_seed = 1,\n",
    "    r = 1\n",
    "):\n",
    "\n",
    "    # Set grid dimensions and random seed\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Step 1: Initialize an empty grid and randomly assign cluster \"patches\"\n",
    "    ground_truth = np.zeros((grid_size, grid_size), dtype=int)\n",
    "    for cluster_id in range(1, num_clusters):\n",
    "        # Randomly choose a center for each cluster\n",
    "        center_x, center_y = np.random.randint(0, grid_size, size=2)\n",
    "        radius = np.random.randint(10, 30)  # Random radius for each cluster region\n",
    "\n",
    "        # Assign cluster_id to a circular region around the chosen center\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if (i - center_x) ** 2 + (j - center_y) ** 2 < radius ** 2:\n",
    "                    ground_truth[i, j] = cluster_id\n",
    "\n",
    "    # Step 2: Add random noise within each patch\n",
    "    noise_level = 0.5\n",
    "    noisy_grid = ground_truth + noise_level * np.random.randn(grid_size, grid_size)\n",
    "\n",
    "    # Step 3: Apply Gaussian smoothing to create spatial clustering\n",
    "    sigma = 3  # Controls the amount of clustering smoothness\n",
    "    smoothed_grid = scipy.ndimage.gaussian_filter(noisy_grid, sigma=sigma)\n",
    "\n",
    "    # Step 4: Threshold to obtain integer values\n",
    "    clustered_grid = np.round(smoothed_grid).astype(int)\n",
    "    clustered_grid = np.clip(clustered_grid, 0, num_clusters)\n",
    "\n",
    "    # Plot the clustered grid with flipped y axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 6)) \n",
    "    ax.imshow(clustered_grid, cmap=\"tab20\", interpolation=\"nearest\", origin='lower')  # Flip y axis by setting origin to 'lower'\n",
    "    ax.set_title(\"Ground Truth Clusters\")\n",
    "    plt.colorbar(ax.imshow(clustered_grid, cmap=\"rainbow\", interpolation=\"nearest\", origin='lower'), ax=ax, label=\"Cluster Level\", ticks=range(num_clusters + 1))  # Flip y axis by setting origin to 'lower'\n",
    "    import os\n",
    "    os.makedirs(\"results/SYNTHETIC\", exist_ok=True)\n",
    "    plt.savefig(\"results/SYNTHETIC/ground_truth.png\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    def find_indices_within_distance(grid, r=1):\n",
    "        indices_within_distance = np.empty((grid.shape[0], grid.shape[1]), dtype=object)\n",
    "        for i in range(grid.shape[0]):\n",
    "            for j in range(grid.shape[1]):\n",
    "                # Check all neighboring cells within a Manhattan distance of 1\n",
    "                neighbors = []\n",
    "                for x in range(max(0, i-r), min(grid.shape[0], i+r+1)):\n",
    "                    for y in range(max(0, j-r), min(grid.shape[1], j+r+1)):\n",
    "                        if abs(x - i) + abs(y - j) <= r:\n",
    "                            neighbors.append((x, y))\n",
    "                indices_within_distance[i, j] = neighbors\n",
    "        return indices_within_distance\n",
    "\n",
    "    # get the rook neighbors for each spot\n",
    "    indices = find_indices_within_distance(clustered_grid, r=r)\n",
    "\n",
    "    prior_weights = np.zeros((clustered_grid.shape[0] * clustered_grid.shape[1], num_clusters))\n",
    "\n",
    "    # for each spot sample \n",
    "    for i in range(clustered_grid.shape[0]):\n",
    "        for j in range(clustered_grid.shape[1]):\n",
    "            for neighbor in indices[i, j]:\n",
    "                prior_weights[i * clustered_grid.shape[1] + j, clustered_grid[neighbor]] += 1\n",
    "    prior_weights = prior_weights / prior_weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Initialize lists for means, covariances, and data points\n",
    "    means = []\n",
    "    covariances = []\n",
    "    data = np.empty((clustered_grid.shape[0] * clustered_grid.shape[1], data_dimension))\n",
    "\n",
    "    # Generate means and covariances for each cluster\n",
    "    for i in range(num_clusters):\n",
    "        # Randomly set the mean close to the origin to encourage overlap\n",
    "        mean = np.random.uniform(-5, 5, data_dimension)\n",
    "        # Generate a diagonal covariance matrix with random magnitudes\n",
    "        covariance = np.diag(np.random.rand(data_dimension) * 2.5)\n",
    "        \n",
    "        means.append(mean)\n",
    "        covariances.append(covariance)\n",
    "\n",
    "    for mean, covariance in zip(means, covariances):\n",
    "        print(mean, covariance)\n",
    "        \n",
    "    # Generate samples from the mixture.\n",
    "    for i, weights in enumerate(prior_weights):\n",
    "        data[i] = np.sum(weights[k] * np.random.multivariate_normal(means[k], covariances[k], 1) for k in range(num_clusters))\n",
    "\n",
    "    # Create an anndata object\n",
    "    adata = ad.AnnData(data)\n",
    "\n",
    "    # Add row and col index\n",
    "    adata.obs['spot_number'] = np.arange(clustered_grid.shape[0] * clustered_grid.shape[1])\n",
    "    adata.obs['spot_number'] = adata.obs['spot_number'].astype('category')\n",
    "    adata.obs['row'] = np.repeat(np.arange(clustered_grid.shape[0]), clustered_grid.shape[1])\n",
    "    adata.obs['col'] = np.tile(np.arange(clustered_grid.shape[1]), clustered_grid.shape[0])\n",
    "    clustering = XeniumCluster(data=adata.X, dataset_name=\"SYNTHETIC\")\n",
    "    clustering.xenium_spot_data = adata\n",
    "\n",
    "    Xenium_to_BayesSpace(clustering.xenium_spot_data, dataset_name=\"SYNTHETIC\", spot_size=grid_size)\n",
    "\n",
    "    return clustering.xenium_spot_data.X, clustering.xenium_spot_data.obs[['row', 'col']], clustering, prior_weights\n",
    "\n",
    "def prepare_DLPFC_data(\n",
    "    section_id=151670,\n",
    "    num_pcs=5,\n",
    "    log_normalize=True,\n",
    "):\n",
    "    section = ad.read_h5ad(f\"data/DLPFC/{section_id}.h5ad\")\n",
    "    section.var[\"feature_name\"] = section.var.index\n",
    "\n",
    "    spatial_locations = section.obs[[\"array_row\", \"array_col\"]]\n",
    "    spatial_locations.columns = [\"row\", \"col\"]\n",
    "\n",
    "    clustering = XeniumCluster(data=section.X, dataset_name=\"DLPFC\")\n",
    "    clustering.xenium_spot_data = section\n",
    "    if log_normalize:\n",
    "        clustering.xenium_spot_data.X = np.log1p(clustering.xenium_spot_data.X)\n",
    "\n",
    "    clustering.pca(clustering.xenium_spot_data, num_pcs)\n",
    "    data = clustering.xenium_spot_data.obsm[\"X_pca\"]\n",
    "\n",
    "    return data, spatial_locations, clustering\n",
    "\n",
    "def prepare_Xenium_data(\n",
    "        dataset=\"hBreast\", \n",
    "        spots=True, \n",
    "        spot_size=100, \n",
    "        third_dim=False, \n",
    "        log_normalize=True,\n",
    "        likelihood_mode=\"PCA\",\n",
    "        num_pcs=5,\n",
    "        hvg_var_prop=0.5,\n",
    "        min_expressions_per_spot=10\n",
    "    ):\n",
    "\n",
    "    data_filepath = f\"data/spot_data/{dataset}/hBreast_SPOTSIZE={spot_size}um_z={third_dim}.h5ad\"\n",
    "    \n",
    "    if spots:\n",
    "\n",
    "        if os.path.exists(data_filepath):\n",
    "\n",
    "            clustering = XeniumCluster(data=None, dataset_name=\"hBreast\")\n",
    "            clustering.set_spot_size(spot_size)\n",
    "            print(\"Loading data.\")\n",
    "            clustering.xenium_spot_data = ad.read_h5ad(data_filepath)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Path to your .gz file\n",
    "            file_path = f'data/{dataset}/transcripts.csv.gz'\n",
    "\n",
    "            # Read the gzipped CSV file into a DataFrame\n",
    "            df_transcripts = pd.read_csv(file_path, compression='gzip')\n",
    "            df_transcripts[\"error_prob\"] = 10 ** (-df_transcripts[\"qv\"]/10)\n",
    "            df_transcripts.head(), df_transcripts.shape\n",
    "\n",
    "            # drop cells without ids\n",
    "            df_transcripts = df_transcripts[df_transcripts[\"cell_id\"] != -1]\n",
    "\n",
    "            # drop blanks and controls\n",
    "            df_transcripts = df_transcripts[~df_transcripts[\"feature_name\"].str.startswith('BLANK_') & ~df_transcripts[\"feature_name\"].str.startswith('NegControl')]\n",
    "\n",
    "            clustering = XeniumCluster(data=df_transcripts, dataset_name=\"hBreast\")\n",
    "            clustering.set_spot_size(spot_size)\n",
    "\n",
    "            if not os.path.exists(data_filepath):\n",
    "                print(\"Generating and saving data\")\n",
    "                clustering.create_spot_data(third_dim=third_dim, save_data=True)\n",
    "                clustering.xenium_spot_data.write_h5ad(data_filepath)\n",
    "\n",
    "        print(\"Number of spots: \", clustering.xenium_spot_data.shape[0])\n",
    "        clustering.xenium_spot_data = clustering.xenium_spot_data[clustering.xenium_spot_data.X.sum(axis=1) > min_expressions_per_spot]\n",
    "        print(\"Number of spots after filtering: \", clustering.xenium_spot_data.shape[0])\n",
    "\n",
    "        if log_normalize:\n",
    "            clustering.normalize_counts(clustering.xenium_spot_data)\n",
    "\n",
    "        if likelihood_mode == \"PCA\":\n",
    "            clustering.pca(clustering.xenium_spot_data, num_pcs)\n",
    "            data = clustering.xenium_spot_data.obsm[\"X_pca\"]\n",
    "        elif likelihood_mode == \"HVG\":\n",
    "            min_dispersion = torch.distributions.normal.Normal(0.0, 1.0).icdf(hvg_var_prop)\n",
    "            clustering.filter_only_high_variable_genes(clustering.xenium_spot_data, flavor=\"seurat\", min_mean=0.0125, max_mean=1000, min_disp=min_dispersion)\n",
    "            clustering.xenium_spot_data = clustering.xenium_spot_data[:,clustering.xenium_spot_data.var.highly_variable==True]\n",
    "            data = clustering.xenium_spot_data.X\n",
    "        elif likelihood_mode == \"ALL\":\n",
    "            data = clustering.xenium_spot_data.X\n",
    "\n",
    "        spatial_locations = clustering.xenium_spot_data.obs[[\"row\", \"col\"]]\n",
    "    \n",
    "    # prepare cells data\n",
    "    else:\n",
    "\n",
    "        cells = df_transcripts.groupby(['cell_id', 'feature_name']).size().reset_index(name='count')\n",
    "        cells_pivot = cells.pivot_table(index='cell_id', \n",
    "                                        columns='feature_name', \n",
    "                                        values='count', \n",
    "                                        fill_value=0)\n",
    "        \n",
    "        location_means = df_transcripts.groupby('cell_id').agg({\n",
    "            'x_location': 'mean',\n",
    "            'y_location': 'mean',\n",
    "            'z_location': 'mean'\n",
    "        }).reset_index()\n",
    "\n",
    "        cells_pivot = location_means.join(cells_pivot, on='cell_id')\n",
    "\n",
    "        if log_normalize:\n",
    "            # log normalization\n",
    "            cells_pivot.iloc[:, 4:] = np.log1p(cells_pivot.iloc[:, 4:])\n",
    "\n",
    "        if likelihood_mode == \"PCA\":\n",
    "            pca = PCA(n_components=num_pcs)\n",
    "            data = pca.fit_transform(cells_pivot.iloc[:, 4:])\n",
    "\n",
    "        elif likelihood_mode == \"HVG\":\n",
    "            genes = cells_pivot.iloc[:, 4:]\n",
    "            gene_variances = genes.var(axis=0)\n",
    "            gene_variances = gene_variances.sort_values(ascending=False)\n",
    "            gene_var_proportions = (gene_variances / sum(gene_variances))\n",
    "            relevant_genes = list(gene_var_proportions[(gene_var_proportions.cumsum() < hvg_var_prop)].index)\n",
    "            cells_pivot.iloc[:, 4:] = cells_pivot.iloc[:, 4:][[relevant_genes]]\n",
    "            data = cells_pivot.iloc[:, 4:]\n",
    "\n",
    "        elif likelihood_mode == \"ALL\":\n",
    "            data = cells_pivot.iloc[:, 4:]\n",
    "\n",
    "        spatial_locations = cells_pivot[[\"x_location\", \"y_location\"]]\n",
    "\n",
    "    # the last one is to regain var/obs access from original data\n",
    "    return data, spatial_locations, clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data, spatial_locations, original_adata, TRUE_WEIGHTS = prepare_synthetic_data()\n",
    "TRUE_ASSIGNMENTS = TRUE_WEIGHTS.argmax(axis=1)\n",
    "spatial_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_size=250\n",
    "data_mode=\"PCA\"\n",
    "num_pcs=3\n",
    "hvg_var_prop=0.9\n",
    "dataset_name=\"SYNTHETIC\"\n",
    "kmeans_init=False\n",
    "custom_init=\"mclust\"\n",
    "spatial_init=True\n",
    "num_clusters=5\n",
    "batch_size=len(gene_data)\n",
    "neighborhood_size=2\n",
    "neighborhood_agg=\"mean\"\n",
    "concentration_amplification=1.0\n",
    "# uncertainty_values = [1/num_clusters, 2/num_clusters, 3/num_clusters, 4/num_clusters, 5/num_clusters]\n",
    "uncertainty_values = [0.25, 0.5, 0.75, 0.9, 0.99]\n",
    "evaluate_markers=False\n",
    "spatial_normalize=0.00\n",
    "learn_global_variances=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_filepath(model, component, sample_for_assignment=None):\n",
    "\n",
    "    total_file_path = (\n",
    "        f\"results/{dataset_name}/{model}/{component}/{data_file_path}/\"\n",
    "        f\"INIT={custom_init}/NEIGHBORSIZE={neighborhood_size}/NUMCLUSTERS={num_clusters}\"\n",
    "        f\"/SPATIALINIT={spatial_init}/SAMPLEFORASSIGNMENT={sample_for_assignment}\"\n",
    "        f\"/SPATIALNORM={spatial_normalize}/SPATIALPRIORMULT={concentration_amplification}/SPOTSIZE={spot_size}/AGG={neighborhood_agg}\"\n",
    "    )\n",
    "\n",
    "    return total_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cluster_initialization(original_adata, method, K=17):\n",
    "\n",
    "    original_adata.generate_neighborhood_graph(original_adata.xenium_spot_data, plot_pcas=False)\n",
    "\n",
    "    # This function initializes clusters based on the specified method\n",
    "    if method == \"K-Means\":\n",
    "        initial_clusters = original_adata.KMeans(original_adata.xenium_spot_data, save_plot=False, K=K, include_spatial=False)\n",
    "    elif method == \"Hierarchical\":\n",
    "        initial_clusters = original_adata.Hierarchical(original_adata.xenium_spot_data, save_plot=True, num_clusters=K)\n",
    "    elif method == \"Leiden\":\n",
    "        initial_clusters = original_adata.Leiden(original_adata.xenium_spot_data, resolutions=[0.35], save_plot=False, K=K)[0.35]\n",
    "    elif method == \"Louvain\":\n",
    "        initial_clusters = original_adata.Louvain(original_adata.xenium_spot_data, resolutions=[0.35], save_plot=False, K=K)[0.35]\n",
    "    elif method == \"mclust\":\n",
    "        original_adata.pca(original_adata.xenium_spot_data, num_pcs)\n",
    "        initial_clusters = original_adata.mclust(original_adata.xenium_spot_data, G=K, model_name = \"EEE\")\n",
    "    elif method == \"random\":\n",
    "        initial_clusters = np.random.randint(0, K, size=original_adata.xenium_spot_data.X.shape[0])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    return initial_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamping\n",
    "MIN_CONCENTRATION = 0.001\n",
    "\n",
    "num_posterior_samples = 1000\n",
    "\n",
    "spatial_init_data = StandardScaler().fit_transform(gene_data)\n",
    "gene_data = StandardScaler().fit_transform(gene_data)\n",
    "empirical_prior_means = torch.zeros(num_clusters, spatial_init_data.shape[1])\n",
    "empirical_prior_scales = torch.ones(num_clusters, spatial_init_data.shape[1])\n",
    "\n",
    "rows = spatial_locations[\"row\"].astype(int)\n",
    "columns = spatial_locations[\"col\"].astype(int)\n",
    "\n",
    "num_rows = max(rows) + 1\n",
    "num_cols = max(columns) + 1\n",
    "\n",
    "if custom_init or kmeans_init:\n",
    "\n",
    "    if custom_init:\n",
    "\n",
    "        initial_clusters = custom_cluster_initialization(original_adata, custom_init, K=num_clusters)\n",
    "        print(custom_init, f\"ARI: {ARI(initial_clusters, TRUE_ASSIGNMENTS)}\", f\"NMI: {NMI(initial_clusters, TRUE_ASSIGNMENTS)}\")\n",
    "\n",
    "    elif kmeans_init:\n",
    "\n",
    "        kmeans_init_data = np.concatenate((spatial_locations, original_adata.xenium_spot_data.X), axis=1)\n",
    "        kmeans_init_data = StandardScaler().fit_transform(kmeans_init_data)\n",
    "\n",
    "        if spatial_normalize:\n",
    "\n",
    "            spatial_dim = spatial_locations.shape[1]\n",
    "            gene_dim = original_adata.xenium_spot_data.X.shape[1]\n",
    "            spatial_factor = (gene_dim * spatial_normalize / (spatial_dim * (1 - spatial_normalize))) ** 0.5\n",
    "            kmeans_init_data[:, :spatial_locations.shape[1]] *= spatial_factor\n",
    "\n",
    "        kmeans = KMeans(n_clusters=num_clusters).fit(kmeans_init_data)\n",
    "\n",
    "        initial_clusters = kmeans.predict(kmeans_init_data)\n",
    "\n",
    "    match data_mode:\n",
    "        case \"PCA\":\n",
    "            data_file_path = f\"{data_mode}/{num_pcs}\"\n",
    "        case \"HVG\": \n",
    "            data_file_path = f\"{data_mode}/{hvg_var_prop}\"\n",
    "        case \"ALL\":\n",
    "            data_file_path = f\"{data_mode}\"\n",
    "        case _:\n",
    "            raise ValueError(\"The data mode specified is not supported.\")\n",
    "        \n",
    "    if kmeans_init and not custom_init:\n",
    "\n",
    "        if not os.path.exists(kmeans_clusters_filepath := save_filepath(\"KMeans\", \"clusters\")):\n",
    "            os.makedirs(kmeans_clusters_filepath)\n",
    "        _ = plt.savefig(\n",
    "            f\"{kmeans_clusters_filepath}/result.png\"\n",
    "        )\n",
    "\n",
    "        cluster_grid = torch.zeros((num_rows, num_cols), dtype=torch.int)\n",
    "        \n",
    "        cluster_grid[rows, columns] = torch.tensor(initial_clusters, dtype=torch.int) + 1\n",
    "\n",
    "        colors = plt.cm.get_cmap('viridis', num_clusters + 1)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(cluster_grid.cpu(), cmap=colors, interpolation='nearest', origin='lower')\n",
    "        plt.colorbar(ticks=range(num_clusters), label='Cluster Values')\n",
    "        plt.title('Cluster Assignment with KMeans')\n",
    "\n",
    "    if dataset_name == \"DLPFC\":\n",
    "        # Create a DataFrame for easier handling\n",
    "        data = pd.DataFrame({\n",
    "            'ClusterAssignments': initial_clusters,\n",
    "            'Region': original_adata.xenium_spot_data.obs[\"Region\"]\n",
    "        })\n",
    "\n",
    "        # Drop rows where 'Region' is NaN\n",
    "        filtered_data = data.dropna(subset=['Region'])\n",
    "\n",
    "        # Calculate ARI and NMI only for the non-NaN entries\n",
    "        ari = ARI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "        nmi = NMI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "        cluster_metrics = {\n",
    "            \"ARI\": ari,\n",
    "            \"NMI\": nmi\n",
    "        }\n",
    "\n",
    "        data_file_path = f\"{data_mode}/{num_pcs}\"\n",
    "\n",
    "        if not os.path.exists(kmeans_cluster_metrics_filepath := save_filepath(\"KMeans\", \"cluster_metrics\")):\n",
    "            os.makedirs(kmeans_cluster_metrics_filepath)\n",
    "        with open(f\"{kmeans_cluster_metrics_filepath}/mpd.json\", 'w') as fp:\n",
    "            json.dump(cluster_metrics, fp)\n",
    "\n",
    "    for i, cluster_value in enumerate(np.unique(initial_clusters)):\n",
    "        cluster_data = gene_data[initial_clusters == cluster_value]\n",
    "        if cluster_data.shape[0] > 0:  # Check if there are any elements in the cluster_data\n",
    "            empirical_prior_means[i] = torch.tensor(cluster_data.mean(axis=0))\n",
    "            empirical_prior_scales[i] = torch.tensor(cluster_data.std(axis=0))\n",
    "        else:\n",
    "            print(\"CLUSTER HAS NO MEMBERS\")\n",
    "    cluster_probs_prior = torch.zeros((initial_clusters.shape[0], num_clusters))\n",
    "    cluster_probs_prior[torch.arange(initial_clusters.shape[0]), initial_clusters - 1] = 1.\n",
    "\n",
    "else:\n",
    "\n",
    "    cluster_probs_prior = torch.ones((len(gene_data), num_clusters), dtype=float)\n",
    "\n",
    "locations_tensor = torch.tensor(spatial_locations.to_numpy())\n",
    "\n",
    "# Compute the number of elements in each dimension\n",
    "num_spots = cluster_probs_prior.shape[0]\n",
    "\n",
    "# Initialize an empty tensor for spatial concentration priors\n",
    "spatial_cluster_probs_prior = torch.zeros_like(cluster_probs_prior, dtype=torch.float64)\n",
    "\n",
    "spot_locations = KDTree(locations_tensor.cpu())  # Ensure this tensor is in host memory\n",
    "neighboring_spot_indexes = spot_locations.query_ball_point(locations_tensor.cpu(), r=neighborhood_size, p=1, workers=8)\n",
    "\n",
    "# Iterate over each spot\n",
    "for i in tqdm(range(num_spots)):\n",
    "\n",
    "    # Select priors in the neighborhood\n",
    "    priors_in_neighborhood = cluster_probs_prior[neighboring_spot_indexes[i]]\n",
    "    # print(f\"Spot {i} has {len(neighboring_spot_indexes[i])} neighbors\")\n",
    "    # print(priors_in_neighborhood)\n",
    "\n",
    "    # Compute the sum or mean, or apply a custom weighting function\n",
    "    if neighborhood_agg == \"mean\":\n",
    "        neighborhood_priors = priors_in_neighborhood.mean(dim=0)\n",
    "    else:\n",
    "        locations = original_adata.xenium_spot_data.obs[[\"x_location\", \"y_location\", \"z_location\"]].values\n",
    "        neighboring_locations = locations[neighboring_spot_indexes[i]].astype(float)\n",
    "        distances = torch.tensor(np.linalg.norm(neighboring_locations - locations[i], axis=1))\n",
    "        def distance_weighting(x):\n",
    "            weight = 1/(1 + x/spot_size)\n",
    "            # print(weight)\n",
    "            return weight / weight.sum()\n",
    "        neighborhood_priors = (priors_in_neighborhood * distance_weighting(distances).reshape(-1, 1)).sum(dim=0)\n",
    "    # Update the cluster probabilities\n",
    "    spatial_cluster_probs_prior[i] += neighborhood_priors\n",
    "\n",
    "spatial_cluster_probs_prior = spatial_cluster_probs_prior.clamp(MIN_CONCENTRATION)\n",
    "sample_for_assignment_options = [True, False]\n",
    "\n",
    "num_prior_samples = 100\n",
    "for sample_for_assignment in sample_for_assignment_options:\n",
    "\n",
    "    if sample_for_assignment:\n",
    "        cluster_assignments_prior_TRUE = pyro.sample(\"cluster_assignments\", dist.Categorical(spatial_cluster_probs_prior).expand_by([num_prior_samples])).detach().mode(dim=0).values\n",
    "        cluster_assignments_prior = cluster_assignments_prior_TRUE\n",
    "    else:\n",
    "        cluster_assignments_prior_FALSE = spatial_cluster_probs_prior.argmax(dim=1)\n",
    "        cluster_assignments_prior = cluster_assignments_prior_FALSE\n",
    "\n",
    "    # Load the data\n",
    "    data = torch.tensor(gene_data).float()\n",
    "\n",
    "    cluster_grid_PRIOR = torch.zeros((num_rows, num_cols), dtype=torch.long)\n",
    "\n",
    "    cluster_grid_PRIOR[rows, columns] = cluster_assignments_prior + 1\n",
    "\n",
    "    colors = plt.cm.get_cmap('viridis', num_clusters)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cluster_grid_PRIOR.cpu(), cmap=colors, interpolation='nearest', origin='lower')\n",
    "    plt.colorbar(ticks=range(1, num_clusters + 1), label='Cluster Values')\n",
    "    plt.title('Prior Cluster Assignment with BayXenSmooth')\n",
    "\n",
    "    if not os.path.exists(bayxensmooth_clusters_filepath := save_filepath(\"BayXenSmooth\", \"clusters\", sample_for_assignment)):\n",
    "        os.makedirs(bayxensmooth_clusters_filepath)\n",
    "    _ = plt.savefig(\n",
    "        f\"{bayxensmooth_clusters_filepath}/prior_result.png\"\n",
    "    )\n",
    "\n",
    "print(\"Spatial_Prior\", f\"ARI: {ARI(cluster_assignments_prior.cpu(), TRUE_ASSIGNMENTS)}\", f\"NMI: {NMI(cluster_assignments_prior.cpu(), TRUE_ASSIGNMENTS)}\")\n",
    "print(empirical_prior_means, empirical_prior_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the mpd distance of cluster labels\n",
    "mpd = {}\n",
    "for label in range(1, num_clusters + 1):\n",
    "    current_cluster_locations = torch.stack(torch.where((cluster_grid_PRIOR.cpu() == label)), axis=1).to(float)\n",
    "    mpd[f\"Cluster {label}\"] = spot_size * torch.mean(torch.cdist(current_cluster_locations, current_cluster_locations, p = 2)).item()\n",
    "sum(mpd.values()) / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "PRIOR_SCALE = np.sqrt(0.1) # higher means weaker\n",
    "NUM_PARTICLES = 25\n",
    "\n",
    "expected_total_param_dim = 2 # K x D\n",
    "\n",
    "def model(data):\n",
    "\n",
    "    with pyro.plate(\"clusters\", num_clusters):\n",
    "\n",
    "        # Define the means and variances of the Gaussian components\n",
    "        cluster_means = pyro.sample(\"cluster_means\", dist.Normal(empirical_prior_means, 2.0).to_event(1))\n",
    "        cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(empirical_prior_scales, 2.0).to_event(1))\n",
    "\n",
    "    # Define priors for the cluster assignment probabilities and Gaussian parameters\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=batch_size) as ind:\n",
    "        batch_data = data[ind]\n",
    "        mu = torch.log(spatial_cluster_probs_prior[ind])\n",
    "        cov_matrix = torch.eye(mu.shape[1], dtype=mu.dtype, device=mu.device) * 10.0\n",
    "        cluster_probs_logits = pyro.sample(\"cluster_logits\", dist.MultivariateNormal(mu, cov_matrix))\n",
    "        cluster_probs = torch.softmax(cluster_probs_logits, dim=-1)\n",
    "        # print(\"MODEL \", cluster_probs[:5])\n",
    "        # likelihood for batch\n",
    "        if cluster_means.dim() == expected_total_param_dim:\n",
    "            pyro.sample(f\"obs\", dist.MixtureOfDiagNormals(\n",
    "                    cluster_means.unsqueeze(0).expand(batch_size, -1, -1), \n",
    "                    cluster_scales.unsqueeze(0).expand(batch_size, -1, -1), \n",
    "                    cluster_probs\n",
    "                ), \n",
    "                obs=batch_data\n",
    "            )\n",
    "        # likelihood for batch WITH vectorization of particles\n",
    "        else:\n",
    "            pyro.sample(f\"obs\", dist.MixtureOfDiagNormals(\n",
    "                    cluster_means.unsqueeze(1).expand(-1, batch_size, -1, -1), \n",
    "                    cluster_scales.unsqueeze(1).expand(-1, batch_size, -1, -1), \n",
    "                    cluster_probs\n",
    "                ),\n",
    "                obs=batch_data\n",
    "            )\n",
    "\n",
    "def guide(data):\n",
    "    # Initialize cluster assignment probabilities for the entire dataset\n",
    "    cluster_probs_logits_q_mean = pyro.param(\"cluster_logits_q_mean\", torch.log(spatial_cluster_probs_prior) + torch.randn_like(spatial_cluster_probs_prior) * 0.1)\n",
    "    cluster_probs_logits_q_scale = pyro.param(\"cluster_logits_q_scale\", torch.ones_like(spatial_cluster_probs_prior, dtype=spatial_cluster_probs_prior.dtype, device=spatial_cluster_probs_prior.device) * 2.5, dist.constraints.positive)\n",
    "\n",
    "    with pyro.plate(\"clusters\", num_clusters):\n",
    "        # Global variational parameters for means and scales\n",
    "        cluster_means_q_mean = pyro.param(\"cluster_means_q_mean\", empirical_prior_means + torch.randn_like(empirical_prior_means) * 0.05)\n",
    "        cluster_scales_q_mean = pyro.param(\"cluster_scales_q_mean\", empirical_prior_scales + torch.randn_like(empirical_prior_scales) * 0.01, constraint=dist.constraints.positive)\n",
    "        if learn_global_variances:\n",
    "            cluster_means_q_scale = pyro.param(\"cluster_means_q_scale\", torch.ones_like(empirical_prior_means) * 1.0, constraint=dist.constraints.positive)\n",
    "            cluster_scales_q_scale = pyro.param(\"cluster_scales_q_scale\", torch.ones_like(empirical_prior_scales) * 0.25, constraint=dist.constraints.positive)\n",
    "            cluster_means = pyro.sample(\"cluster_means\", dist.Normal(cluster_means_q_mean, cluster_means_q_scale).to_event(1))\n",
    "            cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(cluster_scales_q_mean, cluster_scales_q_scale).to_event(1))\n",
    "        else:\n",
    "            cluster_means = pyro.sample(\"cluster_means\", dist.Delta(cluster_means_q_mean).to_event(1))\n",
    "            cluster_scales = pyro.sample(\"cluster_scales\", dist.Delta(cluster_scales_q_mean).to_event(1))\n",
    "\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=batch_size) as ind:\n",
    "\n",
    "        batch_probs_logits_q_mean = cluster_probs_logits_q_mean[ind]\n",
    "        batch_probs_logits_q_scale = cluster_probs_logits_q_scale[ind]\n",
    "        logits = pyro.sample(\"cluster_logits\", dist.Normal(batch_probs_logits_q_mean, batch_probs_logits_q_scale).to_event(1))\n",
    "        cluster_probs = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        # print(\"GUIDE \", cluster_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_prior_means.shape, empirical_prior_scales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyro.render_model(model, model_args=(data,), render_distributions=True, render_params=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "NUM_EPOCHS = 100000\n",
    "NUM_BATCHES = int(math.ceil(data.shape[0] / batch_size))\n",
    "\n",
    "def per_param_callable(param_name):\n",
    "    if param_name == 'cluster_means_q_mean':\n",
    "        return {\"lr\": 0.0075, \"betas\": (0.9, 0.999)}\n",
    "    elif param_name == 'cluster_scales_q_mean':\n",
    "        return {\"lr\": 0.0075, \"betas\": (0.9, 0.999)}\n",
    "    elif \"logit\" in param_name:\n",
    "        return {\"lr\": 0.001, \"betas\": (0.9, 0.999)}\n",
    "    else:\n",
    "        return {\"lr\": 0.005, \"betas\": (0.9, 0.999)}\n",
    "\n",
    "scheduler = Adam(per_param_callable)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "svi = SVI(model, guide, scheduler, loss=TraceMeanField_ELBO(num_particles=NUM_PARTICLES, vectorize_particles=True))\n",
    "\n",
    "# Create a DataLoader for the data\n",
    "data = data.to('cuda')  # Convert data to CUDA tensors\n",
    "\n",
    "# Clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "epoch_pbar = tqdm(range(NUM_EPOCHS))  # Progress bar for epochs\n",
    "cluster_means_trace = []\n",
    "cluster_scales_trace = []\n",
    "current_min_loss = float('inf')\n",
    "PATIENCE = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in epoch_pbar:  # Use tqdm for epoch progress\n",
    "    epoch_pbar.set_description(f\"Epoch {epoch + 1}\")  # Update description with current epoch\n",
    "    running_loss = 0.0\n",
    "    for step in range(NUM_BATCHES):\n",
    "        loss = svi.step(data)\n",
    "        running_loss += loss / batch_size\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        epoch_pbar.set_postfix(loss=round(running_loss, 4))  # Show loss in progress bar\n",
    "        current_cluster_means = pyro.param(\"cluster_means_q_mean\").detach().cpu().numpy()\n",
    "        cluster_means_trace.append(current_cluster_means)\n",
    "        current_cluster_scales = pyro.param(\"cluster_scales_q_mean\").detach().cpu().numpy()\n",
    "        cluster_scales_trace.append(current_cluster_scales)\n",
    "\n",
    "        if running_loss > current_min_loss:\n",
    "            patience_counter += 1\n",
    "        else:\n",
    "            current_min_loss = running_loss\n",
    "            patience_counter = 0\n",
    "        if patience_counter >= PATIENCE:\n",
    "            break \n",
    "\n",
    "        l2_norm = torch.norm(torch.softmax(pyro.param(\"cluster_logits_q_mean\").detach(), dim=-1) - spatial_cluster_probs_prior, p=2)\n",
    "\n",
    "        if dataset_name == \"DLPFC\":\n",
    "            cluster_data = pd.DataFrame({\n",
    "                'ClusterAssignments': cluster_assignments_q,\n",
    "                'Region': original_adata.xenium_spot_data.obs[\"Region\"]\n",
    "            })\n",
    "            filtered_data = cluster_data.dropna(subset=['Region'])\n",
    "            ari = ARI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "            nmi = NMI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "            epoch_pbar.set_postfix(ARI=ari, NMI=nmi)  # Update tqdm with ARI and NMI\n",
    "\n",
    "        elif dataset_name == \"SYNTHETIC\":\n",
    "            num_posterior_samples = 1000\n",
    "            sample_for_assignment_options = [True, False]\n",
    "            for sample_for_assignment in sample_for_assignment_options:\n",
    "                cluster_logits_q_mean = pyro.param(\"cluster_logits_q_mean\")\n",
    "                cluster_logits_q_scale = pyro.param(\"cluster_logits_q_scale\")\n",
    "                if sample_for_assignment:\n",
    "                    cluster_probs_q = torch.softmax(pyro.sample(\"cluster_probs\", dist.Normal(cluster_logits_q_mean, cluster_logits_q_scale).expand_by([num_posterior_samples]).to_event(1)).mean(dim=0), dim=-1)\n",
    "                    cluster_assignments_q = pyro.sample(\"cluster_assignments\", dist.Categorical(cluster_probs_q).expand_by([num_posterior_samples])).mode(dim=0).values\n",
    "                    cluster_assignments_prior = cluster_assignments_prior_TRUE\n",
    "                else:\n",
    "                    cluster_probs_q = torch.softmax(cluster_logits_q_mean, dim=-1)\n",
    "                    cluster_assignments_q = cluster_probs_q.argmax(dim=1)\n",
    "                    cluster_assignments_prior = cluster_assignments_prior_FALSE\n",
    "                cluster_assignments_q_np = cluster_assignments_q.detach().cpu().numpy()\n",
    "                ari = ARI(cluster_assignments_q_np, TRUE_ASSIGNMENTS)\n",
    "                nmi = NMI(cluster_assignments_q_np, TRUE_ASSIGNMENTS)\n",
    "                epoch_pbar.set_postfix(ARI=round(ari, 5), NMI=round(nmi, 5))  # Update tqdm with ARI and NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Appendix Plot Version\n",
    "\n",
    "# Spatial KL Calculation\n",
    "CLAMP = MIN_CONCENTRATION\n",
    "cluster_probs_prior = spatial_cluster_probs_prior.cpu().detach().clamp(CLAMP)\n",
    "TRUE_WEIGHTS = torch.tensor(TRUE_WEIGHTS).cpu().detach().clamp(CLAMP)\n",
    "col_ind = match_labels(TRUE_WEIGHTS.float(), cluster_probs_prior.float())\n",
    "cluster_probs_prior = cluster_probs_prior[:, torch.tensor(col_ind, device=cluster_probs_prior.device)]\n",
    "kl_prior_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / cluster_probs_prior), dim=1)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "NUM_EPOCHS = 2500\n",
    "NUM_BATCHES = int(math.ceil(data.shape[0] / batch_size))\n",
    "\n",
    "def per_param_callable(param_name):\n",
    "    if param_name == 'cluster_means_q_mean':\n",
    "        return {\"lr\": 0.001, \"betas\": (0.9, 0.999)}\n",
    "    elif param_name == 'cluster_scales_q_mean':\n",
    "        return {\"lr\": 0.001, \"betas\": (0.9, 0.999)}\n",
    "    elif \"logit\" in param_name:\n",
    "        return {\"lr\": 0.001, \"betas\": (0.9, 0.999)}\n",
    "    else:\n",
    "        return {\"lr\": 0.005, \"betas\": (0.9, 0.999)}\n",
    "\n",
    "scheduler = Adam(per_param_callable)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "svi = SVI(model, guide, scheduler, loss=TraceMeanField_ELBO(num_particles=NUM_PARTICLES, vectorize_particles=True))\n",
    "\n",
    "# Create a DataLoader for the data\n",
    "# Convert data to CUDA tensors before creating the DataLoader\n",
    "data = data.to('cuda')\n",
    "\n",
    "NUM_ITERATIONS = 25\n",
    "kl_array = []\n",
    "mae_arrays = []\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "\n",
    "    # Clear the param store in case we're in a REPL\n",
    "    pyro.clear_param_store()\n",
    "    kl_array.append([kl_prior_divergence.mean().item()])\n",
    "    mae_arrays.append([])\n",
    "    mae_array = mae_arrays[-1]\n",
    "    current_kl_array = kl_array[-1]\n",
    "    epoch_pbar = tqdm(range(NUM_EPOCHS))\n",
    "    cluster_means_trace = []\n",
    "    cluster_scales_trace = []\n",
    "    current_min_loss = float('inf')\n",
    "    PATIENCE = 5\n",
    "    patience_counter = 0\n",
    "    for epoch in epoch_pbar:  # Use tqdm to wrap the epoch range\n",
    "        # epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
    "        running_loss = 0.0\n",
    "        for step in range(NUM_BATCHES):\n",
    "            loss = svi.step(data)\n",
    "            running_loss += loss / batch_size\n",
    "        # svi.optim.step()\n",
    "        if epoch % 1 == 0:\n",
    "            # print(f\"Epoch {epoch} : loss = {round(running_loss, 4)}\")\n",
    "            current_cluster_means = pyro.param(\"cluster_means_q_mean\").detach().cpu().numpy()\n",
    "            cluster_means_trace.append(current_cluster_means)\n",
    "            current_cluster_scales = pyro.param(\"cluster_scales_q_mean\").detach().cpu().numpy()\n",
    "            cluster_scales_trace.append(current_cluster_scales)\n",
    "            # print(current_cluster_means[0])\n",
    "            if running_loss > current_min_loss:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                current_min_loss = running_loss\n",
    "                patience_counter = 0\n",
    "            if patience_counter >= PATIENCE:\n",
    "                break \n",
    "            l2_norm = torch.norm(torch.softmax(pyro.param(\"cluster_logits_q_mean\").detach(), dim=-1) - spatial_cluster_probs_prior, p=2)\n",
    "            mean_shift = torch.norm(pyro.param(\"cluster_means_q_mean\") - empirical_prior_means, p=2, dim=-1).mean()\n",
    "            scale_shift = torch.norm(pyro.param(\"cluster_scales_q_mean\") - empirical_prior_scales, p=2, dim=-1).mean()\n",
    "            print(f\"Mean Shift: {mean_shift.item()}, Scale Shift: {scale_shift.item()}\")\n",
    "\n",
    "            # print(torch.softmax(pyro.param(\"cluster_logits_q_mean\").detach(), dim=-1)[0])\n",
    "\n",
    "            if dataset_name == \"DLPFC\":\n",
    "                # Create a DataFrame for easier handling\n",
    "                cluster_data = pd.DataFrame({\n",
    "                    'ClusterAssignments': cluster_assignments_q,\n",
    "                    'Region': original_adata.xenium_spot_data.obs[\"Region\"]\n",
    "                })\n",
    "\n",
    "                # Drop rows where 'Region' is NaN\n",
    "                filtered_data = cluster_data.dropna(subset=['Region'])\n",
    "\n",
    "                # Calculate ARI and NMI only for the non-NaN entries\n",
    "                ari = ARI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "                nmi = NMI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "                epoch_pbar.set_description(f\"Epoch {epoch}, ARI {round(ari, 3)}, NMI {round(nmi, 3)}\")\n",
    "                # print(f\"Epoch {epoch} : ARI = {ari} NMI = {nmi}\")\n",
    "\n",
    "            elif dataset_name == \"SYNTHETIC\":\n",
    "                num_posterior_samples = 1000\n",
    "                sample_for_assignment_options = [True, False]\n",
    "                for sample_for_assignment in sample_for_assignment_options:\n",
    "                    cluster_logits_q_mean = pyro.param(\"cluster_logits_q_mean\")\n",
    "                    cluster_logits_q_scale = pyro.param(\"cluster_logits_q_scale\")\n",
    "                    if sample_for_assignment:\n",
    "                        cluster_probs_q = torch.softmax(pyro.sample(\"cluster_probs\", dist.Normal(cluster_logits_q_mean, cluster_logits_q_scale).expand_by([num_posterior_samples]).to_event(1)).mean(dim=0), dim=-1)\n",
    "                        cluster_assignments_q = pyro.sample(\"cluster_assignments\", dist.Categorical(cluster_probs_q).expand_by([num_posterior_samples])).mode(dim=0).values\n",
    "                        cluster_assignments_prior = cluster_assignments_prior_TRUE\n",
    "                    else:\n",
    "                        cluster_probs_q = torch.softmax(cluster_logits_q_mean, dim=-1)\n",
    "                        cluster_assignments_q = cluster_probs_q.argmax(dim=1)\n",
    "                        cluster_assignments_prior = cluster_assignments_prior_FALSE\n",
    "                    # Detach tensors and convert them to numpy before calling ARI and NMI functions\n",
    "                    cluster_assignments_q_np = cluster_assignments_q.detach().cpu().numpy()\n",
    "                    ari = ARI(cluster_assignments_q_np, TRUE_ASSIGNMENTS)\n",
    "                    nmi = NMI(cluster_assignments_q_np, TRUE_ASSIGNMENTS)\n",
    "                    epoch_pbar.set_description(f\"Epoch {epoch}, ARI {round(ari, 3)}, NMI {round(nmi, 3)}\")\n",
    "                    # print(f\"Epoch {epoch}, {sample_for_assignment} : ARI = {round(ari, 2)} NMI = {round(nmi, 2)}\")\n",
    "\n",
    "        cluster_probs_posterior = cluster_probs_q.clamp(CLAMP).detach().cpu()\n",
    "        # KL Calculation\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            col_ind = match_labels(TRUE_WEIGHTS.cpu().float(), cluster_probs_posterior.cpu().float())\n",
    "            cluster_probs_posterior = cluster_probs_posterior[:, torch.tensor(col_ind, device=cluster_probs_posterior.device)]\n",
    "            kl_post_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / cluster_probs_posterior), dim=1)\n",
    "            current_kl_array.append(kl_post_divergence.mean().item())\n",
    "            \n",
    "        else:\n",
    "            cluster_probs_posterior = cluster_probs_posterior[:, torch.tensor(col_ind, device=cluster_probs_prior.device)].detach().cpu()\n",
    "            mae = torch.sum(torch.abs(TRUE_WEIGHTS - cluster_probs_posterior))  # Calculate Mean Absolute Error (MAE)\n",
    "            mae_array.append(mae.item())  # Store MAE in the list\n",
    "            kl_post_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / cluster_probs_posterior), dim=1)\n",
    "            current_kl_array.append(kl_post_divergence.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for the plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx, kl_run in enumerate(kl_array):\n",
    "    plt.plot(kl_run, label=f'Sample {idx}')  # Plot each KL run as a line\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel(r'$\\operatorname{KL}(p || q)$', fontsize=14)\n",
    "plt.title('KL Divergence over Epochs', fontsize=16)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Spatial Empirical Prior', xy=(0, kl_array[0][0]), xytext=(3, 0.051),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12)\n",
    "plt.annotate('Initial Tensors', xy=(1, 0.0675), xytext=(30, 0.0625),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05), fontsize=12)\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('results/KL.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mae in mae_arrays:\n",
    "    plt.plot(mae, label=f'Sample {mae_arrays.index(mae)}')  # Plot each KL run as a line\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(r'$\\operatorname{KL}(p || q)$')\n",
    "plt.title('MAE over Epochs')\n",
    "# plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('results/MAE.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(pyro.param(\"cluster_means_q_mean\").detach() - empirical_prior_means).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param(\"cluster_means_q_mean\").detach(), empirical_prior_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_means_trace = np.array(cluster_means_trace)  # Shape: [num_epochs, num_clusters, data_dim]\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# num_epochs, num_clusters, data_dim = cluster_means_trace.shape\n",
    "\n",
    "# for cluster_idx in range(num_clusters):\n",
    "#     for dim_idx in range(data_dim):\n",
    "#         plt.plot(\n",
    "#             range(num_epochs),\n",
    "#             cluster_means_trace[:, cluster_idx, dim_idx],\n",
    "#             label=f\"Cluster {cluster_idx}, Dim {dim_idx}\"\n",
    "#         )\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(\"Cluster Mean Value\")\n",
    "#         plt.title(f\"Trace Plot for Cluster {cluster_idx}, Dimension {dim_idx}\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_scales_trace = np.array(cluster_scales_trace)  # Shape: [num_epochs, num_clusters, data_dim]\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# num_epochs, num_clusters, data_dim = cluster_scales_trace.shape\n",
    "\n",
    "# for cluster_idx in range(num_clusters):\n",
    "#     for dim_idx in range(data_dim):\n",
    "#         plt.plot(\n",
    "#             range(num_epochs),\n",
    "#             cluster_scales_trace[:, cluster_idx, dim_idx],\n",
    "#             label=f\"Cluster {cluster_idx}, Dim {dim_idx}\"\n",
    "#         )\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(\"Cluster Scale Value\")\n",
    "#         plt.title(f\"Trace Plot for Cluster {cluster_idx}, Dimension {dim_idx}\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# Grab the learned variational parameters\n",
    "num_posterior_samples = 1000\n",
    "sample_for_assignment_options = [True, False]\n",
    "\n",
    "for sample_for_assignment in sample_for_assignment_options:\n",
    "    cluster_logits_q_mean = pyro.param(\"cluster_logits_q_mean\")\n",
    "    cluster_logits_q_scale = pyro.param(\"cluster_logits_q_scale\")\n",
    "    if sample_for_assignment:\n",
    "        cluster_probs_q = torch.softmax(pyro.sample(\"cluster_probs\", dist.Normal(cluster_logits_q_mean, cluster_logits_q_scale).expand_by([num_posterior_samples]).to_event(1)).mean(dim=0), dim=-1)\n",
    "        cluster_assignments_q = pyro.sample(\"cluster_assignments\", dist.Categorical(cluster_probs_q).expand_by([num_posterior_samples])).mode(dim=0).values\n",
    "        cluster_assignments_prior = cluster_assignments_prior_TRUE\n",
    "    else:\n",
    "        cluster_probs_q = torch.softmax(cluster_logits_q_mean, dim=-1)\n",
    "        cluster_assignments_q = cluster_probs_q.argmax(dim=1)\n",
    "        cluster_assignments_prior = cluster_assignments_prior_FALSE\n",
    "    \n",
    "    cluster_means_q_mean = pyro.param(\"cluster_means_q_mean\").cpu().detach()\n",
    "    cluster_scales_q_mean = pyro.param(\"cluster_scales_q_mean\").cpu().detach()\n",
    "    cluster_probs_q = cluster_probs_q.cpu().detach()\n",
    "    cluster_assignments_q = cluster_assignments_q.cpu().detach()\n",
    "    cluster_assignments_prior = cluster_assignments_prior.cpu().detach()\n",
    "\n",
    "    # Plotting\n",
    "    if spot_size:\n",
    "\n",
    "        rows = spatial_locations[\"row\"].astype(int)\n",
    "        columns = spatial_locations[\"col\"].astype(int)\n",
    "\n",
    "        num_rows = max(rows) + 1\n",
    "        num_cols = max(columns) + 1\n",
    "\n",
    "        cluster_grid = torch.zeros((num_rows, num_cols), dtype=torch.long)\n",
    "\n",
    "        cluster_grid[rows, columns] = cluster_assignments_q + 1\n",
    "\n",
    "        colors = plt.cm.get_cmap('viridis', num_clusters + 1)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(cluster_grid.cpu(), cmap=colors, interpolation='nearest', origin='lower')\n",
    "        plt.colorbar(ticks=range(1, num_clusters + 1), label='Cluster Values')\n",
    "        plt.title('Posterior Cluster Assignment with BayXenSmooth')\n",
    "\n",
    "        match data_mode:\n",
    "            case \"PCA\":\n",
    "                data_file_path = f\"{data_mode}/{num_pcs}\"\n",
    "            case \"HVG\": \n",
    "                data_file_path = f\"{data_mode}/{hvg_var_prop}\"\n",
    "            case \"ALL\":\n",
    "                data_file_path = f\"{data_mode}\"\n",
    "            case _:\n",
    "                raise ValueError(\"The data mode specified is not supported.\")\n",
    "\n",
    "        if not os.path.exists(bayxensmooth_clusters_filepath := save_filepath(\"BayXenSmooth\", \"clusters\", sample_for_assignment)):\n",
    "            os.makedirs(bayxensmooth_clusters_filepath)\n",
    "        _ = plt.savefig(\n",
    "            f\"{bayxensmooth_clusters_filepath}/result.png\"\n",
    "        )\n",
    "\n",
    "        clusters = pd.DataFrame(cluster_assignments_q.cpu(), columns=[\"BayXenSmooth cluster\"]).to_csv(f\"{bayxensmooth_clusters_filepath}/clusters_K={num_clusters}.csv\")\n",
    "        soft_clusters = pd.DataFrame(cluster_probs_q, columns=[f'P(z_i = {i})'  for i in range(1, num_clusters + 1)]).to_csv(f\"{bayxensmooth_clusters_filepath}/soft_clusters_K={num_clusters}.csv\")\n",
    "\n",
    "        if not os.path.exists(bayxensmooth_similar_filepath := save_filepath(\"BayXenSmooth\", \"prior_v_posterior\", sample_for_assignment)):\n",
    "            os.makedirs(bayxensmooth_similar_filepath)\n",
    "        with open(f\"{bayxensmooth_similar_filepath}/similarity.txt\", 'w') as fp:\n",
    "            prior_similarity = torch.mean((cluster_assignments_prior == cluster_assignments_q).float()).item()\n",
    "            fp.write(str(prior_similarity))\n",
    "        print(\"PRIOR ARI\", ARI(cluster_assignments_prior, TRUE_ASSIGNMENTS))\n",
    "        print(\"POSTERIOR ARI\", ARI(cluster_assignments_q, TRUE_ASSIGNMENTS))\n",
    "\n",
    "        # grab the mpd distance of cluster labels\n",
    "        mpd = {}\n",
    "        for label in range(1, num_clusters + 1):\n",
    "            current_cluster_locations = torch.stack(torch.where((cluster_grid.cpu() == label)), axis=1).to(float)\n",
    "            mpd[f\"Cluster {label}\"] = spot_size * torch.mean(torch.cdist(current_cluster_locations, current_cluster_locations, p = 2)).item()\n",
    "        print(\"mpd\", sum(mpd.values()) / 1_000_000)\n",
    "\n",
    "        if not os.path.exists(bayxensmooth_mpd_filepath := save_filepath(\"BayXenSmooth\", \"mpd\", sample_for_assignment)):\n",
    "            os.makedirs(bayxensmooth_mpd_filepath)\n",
    "        with open(f\"{bayxensmooth_mpd_filepath}/mpd.json\", 'w') as fp:\n",
    "            json.dump(mpd, fp)\n",
    "\n",
    "        cmap = get_cmap('rainbow')\n",
    "\n",
    "        if isinstance(original_adata.xenium_spot_data.X, csr_matrix):\n",
    "            labels = np.unique(cluster_assignments_q)  # Define the number of clusters\n",
    "            gene_columns = original_adata.xenium_spot_data.var.index  # Column names from another source\n",
    "            mean_expression_by_cluster = pd.DataFrame(columns=gene_columns)\n",
    "\n",
    "            # Loop through each cluster label\n",
    "            for label in labels:\n",
    "                # Find indexes of current cluster\n",
    "                current_cluster_indexes = torch.where(cluster_assignments_q == label)[0].numpy()\n",
    "                \n",
    "                # Efficiently extract the rows for the current cluster using fancy indexing\n",
    "                expressions = original_adata.xenium_spot_data.X[current_cluster_indexes, :]\n",
    "                print(label, expressions.shape)\n",
    "                \n",
    "                # Compute mean expressions; the result is still a csr_matrix\n",
    "                mean_expressions = expressions.mean(axis=0)\n",
    "                \n",
    "                # Convert mean_expressions to a dense format and then to a DataFrame\n",
    "                mean_expressions_df = pd.DataFrame(mean_expressions.A, columns=gene_columns)\n",
    "                \n",
    "                # Append the result to the mean_expression_by_cluster DataFrame\n",
    "                mean_expression_by_cluster = pd.concat([mean_expression_by_cluster, mean_expressions_df], ignore_index=True)\n",
    "        else:\n",
    "            # identify marker genes within each cluster\n",
    "            mean_expression_by_cluster = pd.DataFrame(columns=original_adata.xenium_spot_data.var.index)\n",
    "\n",
    "            for label in range(num_clusters):\n",
    "                current_cluster_indexes = list(torch.where(cluster_assignments_q == label)[0].cpu().numpy())\n",
    "                expressions = pd.DataFrame(original_adata.xenium_spot_data.X, columns=original_adata.xenium_spot_data.var.index).iloc[current_cluster_indexes, :]\n",
    "                mean_expressions = expressions.mean(axis=0).to_frame().T\n",
    "                mean_expression_by_cluster = pd.concat([mean_expression_by_cluster, mean_expressions], ignore_index=True)\n",
    "\n",
    "        if evaluate_markers:\n",
    "            for i, gene in enumerate(mean_expression_by_cluster.columns):\n",
    "                # using subplots() to draw vertical lines \n",
    "                fig, ax = plt.subplots(figsize=(6, 6)) \n",
    "                ax.vlines(mean_expression_by_cluster[gene].index, ymin=0, ymax=mean_expression_by_cluster[gene], color=cmap(i / (len(mean_expression_by_cluster.columns) - 1))) \n",
    "                \n",
    "                # drawing the markers\n",
    "                ax.plot(mean_expression_by_cluster[gene].index, mean_expression_by_cluster[gene], \"^\", c=cmap(i / (len(mean_expression_by_cluster.columns) - 1))) \n",
    "                ax.set_ylim(0) \n",
    "                \n",
    "                # formatting and details \n",
    "                ax.set_xlabel('Cluster Label') \n",
    "                ax.set_ylabel('Mean Expression') \n",
    "                ax.set_title(gene) \n",
    "                ax.set_xticks(mean_expression_by_cluster[gene].index) \n",
    "                if not os.path.exists(bayxensmooth_expression_filepath := save_filepath(\"BayXenSmooth\", \"expressions\", sample_for_assignment)):\n",
    "                    os.makedirs(f\"{bayxensmooth_expression_filepath}\")\n",
    "                _ = plt.savefig(\n",
    "                    f\"{bayxensmooth_expression_filepath}/GENE={gene}.png\"\n",
    "                )\n",
    "        \n",
    "        # confidence mapping\n",
    "        cluster_confidences = torch.zeros((num_rows, num_cols), dtype=torch.double)\n",
    "\n",
    "        cluster_confidences[rows, columns] = cluster_probs_q.max(dim=1).values\n",
    "\n",
    "        heatmap_bins = 21\n",
    "        colors = plt.cm.get_cmap('YlOrRd', heatmap_bins)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(cluster_confidences, cmap=colors, interpolation='nearest', origin='lower')\n",
    "        # plt.xticks([])  # Remove x-axis tick marks\n",
    "        # plt.yticks([])  # Remove y-axis tick marks\n",
    "        plt.gca().spines['top'].set_visible(False)  # Remove top border\n",
    "        plt.gca().spines['right'].set_visible(False)  # Remove right border\n",
    "        # plt.gca().spines['bottom'].set_visible(False)  # Remove bottom border\n",
    "        # plt.gca().spines['left'].set_visible(False)  # Remove left border\n",
    "        cbar = plt.colorbar(fraction=0.046, pad=0.04)  # Make colorbar the same height as the figure\n",
    "        plt.title(r'$P(z_i = k)$')\n",
    "\n",
    "\n",
    "        colors = plt.cm.get_cmap('Greys', num_clusters)\n",
    "        colormap = ListedColormap(colors(np.linspace(0, 1, num_clusters)))\n",
    "\n",
    "        confidence_proportions = {}\n",
    "        for uncertainty_value in uncertainty_values:\n",
    "            confidence_matrix = (cluster_confidences > uncertainty_value).float()\n",
    "            confidence_proportions[uncertainty_value] = torch.mean(confidence_matrix).item()\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(cluster_confidences > uncertainty_value, cmap=colormap, interpolation='nearest', origin='lower')\n",
    "            # plt.xticks([])  # Remove x-axis tick marks\n",
    "            # plt.yticks([])  # Remove y-axis tick marks\n",
    "            plt.gca().spines['top'].set_visible(False)  # Remove top border\n",
    "            plt.gca().spines['right'].set_visible(False)  # Remove right border\n",
    "            # plt.gca().spines['bottom'].set_visible(False)  # Remove bottom border\n",
    "            # plt.gca().spines['left'].set_visible(False)  # Remove left border\n",
    "            cbar = plt.colorbar(fraction=0.046, pad=0.04)  # Make colorbar the same height as the figure\n",
    "            # PLOT ALL UNCERTAINTY VALUESs\n",
    "            plt.title(r'$\\max_k \\, P(z_i = k) > $' + f'{uncertainty_value}')\n",
    "            if not os.path.exists(bayxensmooth_uncertainty_filepath := save_filepath(\"BayXenSmooth\", \"uncertainty\", sample_for_assignment)):\n",
    "                os.makedirs(bayxensmooth_uncertainty_filepath)\n",
    "            _ = plt.savefig(\n",
    "                f\"{bayxensmooth_uncertainty_filepath}/CONFIDENCE={uncertainty_value}.png\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "\n",
    "        plt.scatter(spatial_locations[\"x_location\"], spatial_locations[\"y_location\"], s=1, c=cluster_assignments_q)\n",
    "        if not os.path.exists(bayxensmooth_clusters_filepath := save_filepath(\"BayXenSmooth\", \"clusters\", sample_for_assignment)):\n",
    "            os.makedirs(bayxensmooth_clusters_filepath)\n",
    "        _ = plt.savefig(\n",
    "            f\"{bayxensmooth_clusters_filepath}/result.png\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL Calculation\n",
    "CLAMP = MIN_CONCENTRATION\n",
    "cluster_probs_prior = cluster_probs_prior.cpu().detach().clamp(CLAMP)\n",
    "cluster_probs_posterior = cluster_probs_q.cpu().detach().clamp(CLAMP)\n",
    "TRUE_WEIGHTS = torch.tensor(TRUE_WEIGHTS).cpu().clamp(CLAMP)\n",
    "col_ind = match_labels(TRUE_WEIGHTS.float(), cluster_probs_prior.float())\n",
    "cluster_probs_prior = cluster_probs_prior[:, torch.tensor(col_ind, device=cluster_probs_prior.device)]\n",
    "col_ind = match_labels(TRUE_WEIGHTS.float(), cluster_probs_posterior.float())\n",
    "cluster_probs_posterior = cluster_probs_posterior[:, torch.tensor(col_ind, device=cluster_probs_prior.device)]\n",
    "\n",
    "\n",
    "kl_prior_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / cluster_probs_prior), dim=1)\n",
    "kl_post_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / cluster_probs_posterior), dim=1)\n",
    "kl_prior_divergence.mean(), kl_post_divergence.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial KL Calculation\n",
    "CLAMP = MIN_CONCENTRATION\n",
    "cluster_probs_prior = spatial_cluster_probs_prior.cpu().detach().clamp(CLAMP)\n",
    "col_ind = match_labels(TRUE_WEIGHTS.float(), cluster_probs_prior.float())\n",
    "cluster_probs_prior = cluster_probs_prior[:, torch.tensor(col_ind, device=cluster_probs_prior.device)]\n",
    "cluster_probs_posterior = cluster_probs_q.cpu().detach().clamp(CLAMP)\n",
    "TRUE_WEIGHTS = torch.tensor(TRUE_WEIGHTS).cpu().clamp(CLAMP)\n",
    "col_ind = match_labels(TRUE_WEIGHTS.float(), cluster_probs_posterior.float())\n",
    "cluster_probs_posterior = cluster_probs_posterior[:, torch.tensor(col_ind, device=cluster_probs_prior.device)]\n",
    "\n",
    "kl_prior_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / cluster_probs_prior), dim=1)\n",
    "kl_post_divergence = torch.sum(TRUE_WEIGHTS * torch.log(TRUE_WEIGHTS / cluster_probs_posterior), dim=1)\n",
    "kl_prior_divergence.mean(), kl_post_divergence.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_WEIGHTS, cluster_probs_posterior, cluster_probs_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((TRUE_WEIGHTS - cluster_probs_posterior) ** 2).sum(), ((TRUE_WEIGHTS - cluster_probs_prior) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_bins = 21\n",
    "colors = plt.cm.get_cmap('YlOrRd', heatmap_bins)\n",
    "colormap_colors = np.vstack(([[1, 1, 1, 1]], colors(np.linspace(0, 1, heatmap_bins - 1))))\n",
    "colormap = ListedColormap(colormap_colors)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cluster_confidences, cmap=colormap, interpolation='nearest', origin='lower')\n",
    "# plt.xticks([])  # Remove x-axis tick marks\n",
    "# plt.yticks([])  # Remove y-axis tick marks\n",
    "plt.gca().spines['top'].set_visible(False)  # Remove top border\n",
    "plt.gca().spines['right'].set_visible(False)  # Remove right border\n",
    "# plt.gca().spines['bottom'].set_visible(False)  # Remove bottom border\n",
    "# plt.gca().spines['left'].set_visible(False)  # Remove left border\n",
    "cbar = plt.colorbar(fraction=0.046, pad=0.04)  # Make colorbar the same height as the figure\n",
    "plt.title(r'$\\max_k P(z_i = k)$')\n",
    "\n",
    "# confidence_proportions = {}\n",
    "# for uncertainty_value in uncertainty_values:\n",
    "#     confidence_matrix = (cluster_confidences > uncertainty_value).float()\n",
    "#     confidence_proportions[uncertainty_value] = torch.mean(confidence_matrix).item()\n",
    "#     plt.figure(figsize=(6, 6))\n",
    "#     plt.imshow(cluster_confidences > uncertainty_value, cmap=colormap, interpolation='nearest', origin='lower')\n",
    "#     plt.colorbar(ticks=range(num_clusters + 1), label='Cluster Values')\n",
    "#     # PLOT ALL UNCERTAINTY VALUESs\n",
    "#     plt.title(r'$P(z_i = k) > $' + f'{uncertainty_value}')\n",
    "#     if not os.path.exists(bayxensmooth_uncertainty_filepath := save_filepath(\"BayXenSmooth\", \"uncertainty\", sample_for_assignment)):\n",
    "#         os.makedirs(bayxensmooth_uncertainty_filepath)\n",
    "#     _ = plt.savefig(\n",
    "#         f\"{bayxensmooth_uncertainty_filepath}/CONFIDENCE={uncertainty_value}.png\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "same_labels = (cluster_grid_PRIOR.cpu() == cluster_grid.cpu()).float() * 2 - 1\n",
    "same_labels[cluster_grid_PRIOR.cpu() == 0] = 0\n",
    "colors = plt.cm.get_cmap('bwr', heatmap_bins)\n",
    "plt.imshow(same_labels, cmap=colors, interpolation='nearest', origin='lower')\n",
    "plt.colorbar(ticks=[-1,0,1], label='Prior = Posterior', fraction=0.046, pad=0.04)  # Make colorbar the same height as the figure\n",
    "print(f\"Proportion of Same Labels: {same_labels.float().mean()}\")\n",
    "_ = plt.title(r'$w_{i, prior} = w_{i, posterior}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = [\n",
    "    [-1.85535937, 4.04186255, -0.95228016, 3.08862541, 0.81053303],\n",
    "    [1.01939909, 1.81791175, -0.75413555, -2.17643892, -3.61932162],\n",
    "    [-2.47550277, -0.45159793, 4.55002626, -0.0224897, -4.2889903],\n",
    "    [-3.49125761, -2.29407797, 2.19677975, -3.69091608, -0.0805174],\n",
    "    [-1.22752138, 3.63399274, -0.85551861, 4.07738352, 3.8109539]\n",
    "]\n",
    "\n",
    "scales = [\n",
    "    [[2.4887799, 0., 0., 0., 0.],\n",
    "     [0., 1.10497931, 0., 0., 0.],\n",
    "     [0., 0., 1.26053098, 0., 0.],\n",
    "     [0., 0., 0., 0.43709112, 0.],\n",
    "     [0., 0., 0., 0., 1.30780163]],\n",
    "    \n",
    "    [[2.37373841, 0., 0., 0., 0.],\n",
    "     [0., 0.80503623, 0., 0., 0.],\n",
    "     [0., 0., 0.05964982, 0., 0.],\n",
    "     [0., 0., 0., 0.33034402, 0.],\n",
    "     [0., 0., 0., 0., 0.35332062]],\n",
    "    \n",
    "    [[0.59966493, 0., 0., 0., 0.],\n",
    "     [0., 1.14816772, 0., 0., 0.],\n",
    "     [0., 0., 1.98996892, 0., 0.],\n",
    "     [0., 0., 0., 0.7057492, 0.],\n",
    "     [0., 0., 0., 0., 1.00689833]],\n",
    "    \n",
    "    [[2.12290974, 0., 0., 0., 0.],\n",
    "     [0., 1.87391377, 0., 0., 0.],\n",
    "     [0., 0., 2.05124718, 0., 0.],\n",
    "     [0., 0., 0., 1.08134021, 0.],\n",
    "     [0., 0., 0., 0., 2.46312849]],\n",
    "    \n",
    "    [[0.6676434, 0., 0., 0., 0.],\n",
    "     [0., 1.97638125, 0., 0., 0.],\n",
    "     [0., 0., 2.44105221, 0., 0.],\n",
    "     [0., 0., 0., 2.22082959, 0.],\n",
    "     [0., 0., 0., 0., 0.75916656]]\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "means_scaled = scaler.fit_transform(np.array(means))\n",
    "means = torch.tensor(means).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_posterior_mean_means = pyro.param(\"cluster_means_q_mean\").detach().cpu().numpy()\n",
    "print(cluster_posterior_mean_means.shape)\n",
    "\n",
    "index_ordering = match_labels(\n",
    "    torch.tensor(empirical_prior_means).T.cpu(),\n",
    "    torch.tensor(cluster_posterior_mean_means).T.cpu()\n",
    ")\n",
    "\n",
    "cluster_posterior_mean_means = cluster_posterior_mean_means[index_ordering]\n",
    "\n",
    "index_ordering = match_labels(\n",
    "    torch.tensor(empirical_prior_means).T.cpu(),\n",
    "    torch.tensor(means).T.cpu()\n",
    ")\n",
    "\n",
    "true_means = np.array(means)[index_ordering]\n",
    "\n",
    "K = cluster_posterior_mean_means.shape[0]  # Number of clusters\n",
    "D = cluster_posterior_mean_means.shape[1]  # Number of dimensions\n",
    "\n",
    "fig, axs = plt.subplots(K, 1, figsize=(10, 2 * K), sharex=True)\n",
    "\n",
    "for k in range(K):\n",
    "    axs[k].plot(range(D), true_means[k], label='True Mean', color='blue', marker='o')\n",
    "    axs[k].plot(range(D), scaler.inverse_transform(empirical_prior_means[k].cpu().reshape(1, -1))[0], label='Empirical Prior Mean', color='orange', marker='x')\n",
    "    axs[k].plot(range(D), scaler.inverse_transform(cluster_posterior_mean_means[k].reshape(1, -1))[0], label='Posterior Mean', color='green', marker='s')\n",
    "    \n",
    "    axs[k].set_title(f'Cluster {k + 1}')\n",
    "    axs[k].set_ylabel('Mean Values')\n",
    "    axs[k].legend()\n",
    "    axs[k].grid()\n",
    "\n",
    "plt.xlabel('Data Dimensions')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/SYNTHETIC/BayXenSmooth_means.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = torch.tensor(scales)\n",
    "print(scales.shape)\n",
    "scales = torch.stack([torch.diag(scales[i]) for i in range(scales.shape[0])])\n",
    "print(scales.shape)\n",
    "\n",
    "cluster_posterior_scale_means = pyro.param(\"cluster_scales_q_mean\").detach().cpu().numpy()\n",
    "print(cluster_posterior_scale_means.shape)\n",
    "\n",
    "index_ordering = match_labels(\n",
    "    torch.tensor(empirical_prior_scales).T.cpu(),\n",
    "    torch.tensor(cluster_posterior_scale_means).T.cpu()\n",
    ")\n",
    "\n",
    "cluster_posterior_scale_means = cluster_posterior_scale_means[index_ordering]\n",
    "\n",
    "index_ordering = match_labels(\n",
    "    torch.tensor(empirical_prior_scales).T.cpu(),\n",
    "    torch.tensor(scales).T.cpu()\n",
    ")\n",
    "\n",
    "true_scales = np.array(scales)[index_ordering]\n",
    "\n",
    "K = cluster_posterior_scale_means.shape[0]  # Number of clusters\n",
    "D = cluster_posterior_scale_means.shape[1]  # Number of dimensions\n",
    "\n",
    "fig, axs = plt.subplots(K, 1, figsize=(10, 2 * K), sharex=True)\n",
    "\n",
    "for k in range(K):\n",
    "    axs[k].plot(range(D), true_scales[k], label='True Scale', color='blue', marker='o')\n",
    "    axs[k].plot(range(D), empirical_prior_scales[k].cpu()  * scaler.scale_, label='Empirical Prior Scale', color='orange', marker='x')\n",
    "    axs[k].plot(range(D), cluster_posterior_scale_means[k] * scaler.scale_, label='Posterior Scale', color='green', marker='s')\n",
    "    \n",
    "    axs[k].set_title(f'Cluster {k + 1}')\n",
    "    axs[k].set_ylabel('Scale Values')\n",
    "    axs[k].legend()\n",
    "    axs[k].grid()\n",
    "\n",
    "plt.xlabel('Data Dimensions')\n",
    "plt.savefig('results/SYNTHETIC/BayXenSmooth_scales.png')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_prior_means, cluster_posterior_mean_means, true_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_prior_scales, cluster_posterior_scale_means, scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xenium-1YUjn3qu-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
