{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from importlib import reload\n",
    "\n",
    "# this ensures that I can update the class without losing my variables in my notebook\n",
    "import xenium_cluster\n",
    "reload(xenium_cluster)\n",
    "from xenium_cluster import XeniumCluster\n",
    "from utils.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/hBreast/transcripts.csv.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Read the gzipped CSV file into a DataFrame\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_transcripts \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgzip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# drop cells without ids\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df_transcripts \u001b[38;5;241m=\u001b[39m df_transcripts[df_transcripts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcell_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/xenium-1YUjn3qu-py3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/xenium-1YUjn3qu-py3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/xenium-1YUjn3qu-py3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/xenium-1YUjn3qu-py3.10/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.10/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/usr/lib/python3.10/gzip.py:496\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[1;32m    494\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[0;32m--> 496\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mprepend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Path to your .gz file\n",
    "file_path = 'data/hBreast/transcripts.csv.gz'\n",
    "\n",
    "# Read the gzipped CSV file into a DataFrame\n",
    "df_transcripts = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "# drop cells without ids\n",
    "df_transcripts = df_transcripts[df_transcripts[\"cell_id\"] != -1]\n",
    "\n",
    "# drop blanks and controls\n",
    "df_transcripts = df_transcripts[~df_transcripts[\"feature_name\"].str.startswith('BLANK_') & ~df_transcripts[\"feature_name\"].str.startswith('NegControl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(data, dataset_name: str, current_spot_size: int, third_dim: bool, resolutions: list, n_clusters=15):\n",
    "    \n",
    "    clustering = XeniumCluster(data=data, dataset_name=dataset_name)\n",
    "    clustering.set_spot_size(current_spot_size)\n",
    "    clustering.create_spot_data(third_dim=third_dim, save_data=True)\n",
    "\n",
    "    print(f\"The size of the spot data is {clustering.xenium_spot_data.shape}\")\n",
    "\n",
    "    clustering.normalize_counts(clustering.xenium_spot_data)\n",
    "    clustering.generate_neighborhood_graph(clustering.xenium_spot_data, plot_pcas=False)\n",
    "\n",
    "    k_means_cluster = clustering.KMeans(clustering.xenium_spot_data, save_plot=True, K=n_clusters)\n",
    "    k_means_cluster_no_spatial = clustering.KMeans(clustering.xenium_spot_data, save_plot=True, K=n_clusters, include_spatial=False)\n",
    "    return clustering, k_means_cluster, k_means_cluster_no_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def record_results(original_data, cluster_dict, results_dir, model_name, filename, spot_size, third_dim, K=None, resolution=None, uses_spatial=True):\n",
    "\n",
    "    dirpath = f\"{results_dir}/{model_name}{'/' + (str(resolution) if resolution is not None else str(K))}/clusters/{spot_size}\"\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "    filepath = f\"{dirpath}/{filename}.json\"\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(cluster_dict[model_name], f, indent=4)\n",
    "\n",
    "    wss = {}\n",
    "    if resolution is not None:\n",
    "        current_clustering = np.array(cluster_dict[model_name][spot_size][third_dim].get(\n",
    "            resolution, \n",
    "            cluster_dict[model_name][spot_size][third_dim]\n",
    "        ))\n",
    "    else:\n",
    "        current_clustering = np.array(cluster_dict[model_name][spot_size][third_dim][uses_spatial].get(\n",
    "            K, \n",
    "            cluster_dict[model_name][spot_size][third_dim][uses_spatial]\n",
    "        ))\n",
    "    cluster_labels = np.unique(current_clustering)\n",
    "\n",
    "    original_data.xenium_spot_data.obs[f\"{model_name} cluster\"] = np.array(current_clustering)\n",
    "    # Extracting row, col, and cluster values from the dataframe\n",
    "    rows = torch.tensor(original_data.xenium_spot_data.obs[\"row\"].astype(int))\n",
    "    cols = torch.tensor(original_data.xenium_spot_data.obs[\"col\"].astype(int))\n",
    "    clusters = torch.tensor(original_data.xenium_spot_data.obs[f\"{model_name} cluster\"].astype(int))\n",
    "    cluster_labels = np.unique(clusters)\n",
    "\n",
    "    num_rows = int(max(rows) - min(rows) + 1)\n",
    "    num_cols = int(max(cols) - min(cols) + 1)\n",
    "\n",
    "    cluster_grid = torch.zeros((num_rows, num_cols), dtype=torch.int)\n",
    "\n",
    "    cluster_grid[rows, cols] = torch.tensor(clusters, dtype=torch.int)\n",
    "\n",
    "    for label in cluster_labels:\n",
    "        current_cluster_locations = torch.stack(torch.where((cluster_grid == label)), axis=1).to(float)\n",
    "        wss[f\"Cluster {label}\"] = (spot_size ** 2) * torch.mean(torch.cdist(current_cluster_locations, current_cluster_locations)).item()\n",
    "        print(f\"POSSIBLE {len(cluster_labels)}\", label, wss[f\"Cluster {label}\"])\n",
    "\n",
    "    wss_dirpath = f\"{results_dir}/{model_name}{'/' + (str(resolution) if resolution is not None else str(K))}/wss/{spot_size}/\"\n",
    "    if not os.path.exists(wss_dirpath):\n",
    "        os.makedirs(wss_dirpath)\n",
    "\n",
    "    wss_filepath = f\"{wss_dirpath}/{filename}_wss.json\"\n",
    "    with open(wss_filepath, \"w\") as f:\n",
    "        json.dump(wss, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dict = {\"K-Means\": {}}\n",
    "wss = {\"K-Means\": {}}\n",
    "results_dir = \"results/hBreast\"\n",
    "cluster_results_filename = \"clusters_w_plots_7_26\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the spot data is (23444, 280)\n",
      "POSSIBLE 17 0 1172364.3847794936\n",
      "POSSIBLE 17 1 811020.6026604249\n",
      "POSSIBLE 17 2 639793.0192512847\n",
      "POSSIBLE 17 3 773928.8500365223\n",
      "POSSIBLE 17 4 706058.1776911407\n",
      "POSSIBLE 17 5 548363.1997773956\n",
      "POSSIBLE 17 6 831113.8356806508\n",
      "POSSIBLE 17 7 664859.1724142379\n",
      "POSSIBLE 17 8 540178.2981053337\n",
      "POSSIBLE 17 9 631367.945356697\n",
      "POSSIBLE 17 10 769052.727951386\n",
      "POSSIBLE 17 11 624569.3298681895\n",
      "POSSIBLE 17 12 649145.7557208883\n",
      "POSSIBLE 17 13 771356.700240844\n",
      "POSSIBLE 17 14 755524.8635735047\n",
      "POSSIBLE 17 15 620518.0253296511\n",
      "POSSIBLE 17 16 764478.8312168405\n",
      "POSSIBLE 17 0 1153176.1198824022\n",
      "POSSIBLE 17 1 698672.123931602\n",
      "POSSIBLE 17 2 760815.7918723203\n",
      "POSSIBLE 17 3 780933.7870439377\n",
      "POSSIBLE 17 4 712962.701778846\n",
      "POSSIBLE 17 5 750078.1272875896\n",
      "POSSIBLE 17 6 567848.4713700826\n",
      "POSSIBLE 17 7 606289.0578445804\n",
      "POSSIBLE 17 8 686506.8608095864\n",
      "POSSIBLE 17 9 810475.9221306513\n",
      "POSSIBLE 17 10 532760.6356304223\n",
      "POSSIBLE 17 11 690054.4507895558\n",
      "POSSIBLE 17 12 621684.2433684391\n",
      "POSSIBLE 17 13 777848.9512311867\n",
      "POSSIBLE 17 14 606714.557027\n",
      "POSSIBLE 17 15 830310.237878534\n",
      "POSSIBLE 17 16 708813.259015096\n",
      "Cluster with spot size (50, False, 17) completed.\n",
      "The size of the spot data is (10734, 280)\n",
      "POSSIBLE 17 0 741768.607939443\n",
      "POSSIBLE 17 1 493541.69383564906\n",
      "POSSIBLE 17 2 452980.2105572755\n",
      "POSSIBLE 17 3 395618.3184754111\n",
      "POSSIBLE 17 4 451150.00560538925\n",
      "POSSIBLE 17 5 526235.9512099744\n",
      "POSSIBLE 17 6 359124.3258489108\n",
      "POSSIBLE 17 7 526756.0860502316\n",
      "POSSIBLE 17 8 408488.85258467996\n",
      "POSSIBLE 17 9 444873.1302469918\n",
      "POSSIBLE 17 10 401687.6577628206\n",
      "POSSIBLE 17 11 422338.5466772903\n",
      "POSSIBLE 17 12 503085.64918294456\n",
      "POSSIBLE 17 13 504304.2596810769\n",
      "POSSIBLE 17 14 517927.348338501\n",
      "POSSIBLE 17 15 448084.1072121701\n",
      "POSSIBLE 17 16 299030.10311280703\n",
      "POSSIBLE 17 0 784864.2750840982\n",
      "POSSIBLE 17 1 530434.7065557548\n",
      "POSSIBLE 17 2 391300.8005585504\n",
      "POSSIBLE 17 3 503014.43008772307\n",
      "POSSIBLE 17 4 470607.4393413291\n",
      "POSSIBLE 17 5 406827.54836674244\n",
      "POSSIBLE 17 6 504741.32751379575\n",
      "POSSIBLE 17 7 533928.7354169864\n",
      "POSSIBLE 17 8 512975.0811894832\n",
      "POSSIBLE 17 9 458226.92245660536\n",
      "POSSIBLE 17 10 411575.95276511915\n",
      "POSSIBLE 17 11 352521.7931811083\n",
      "POSSIBLE 17 12 417566.83321508364\n",
      "POSSIBLE 17 13 518467.92560334783\n",
      "POSSIBLE 17 14 508526.79417699465\n",
      "POSSIBLE 17 15 416159.89230652346\n",
      "POSSIBLE 17 16 320584.7501924634\n",
      "Cluster with spot size (75, False, 17) completed.\n",
      "The size of the spot data is (6138, 280)\n",
      "POSSIBLE 17 0 577105.5735131849\n",
      "POSSIBLE 17 1 310766.91358650546\n",
      "POSSIBLE 17 2 244337.65241083558\n",
      "POSSIBLE 17 3 325017.31571887544\n",
      "POSSIBLE 17 4 381802.72363609297\n",
      "POSSIBLE 17 5 301609.5028896957\n",
      "POSSIBLE 17 6 341707.0422842694\n",
      "POSSIBLE 17 7 336261.60608799104\n",
      "POSSIBLE 17 8 385030.53517234285\n",
      "POSSIBLE 17 9 369602.73215715255\n",
      "POSSIBLE 17 10 379577.75430971046\n",
      "POSSIBLE 17 11 254440.39667801053\n",
      "POSSIBLE 17 12 377232.2294926618\n",
      "POSSIBLE 17 13 289764.4366214511\n",
      "POSSIBLE 17 14 296126.00111408794\n",
      "POSSIBLE 17 15 423323.7718590384\n",
      "POSSIBLE 17 16 282993.55744296784\n",
      "POSSIBLE 17 0 589109.9109664313\n",
      "POSSIBLE 17 1 306655.659152273\n",
      "POSSIBLE 17 2 396229.63996108377\n",
      "POSSIBLE 17 3 401605.99122855265\n",
      "POSSIBLE 17 4 291719.5393860371\n",
      "POSSIBLE 17 5 379254.2489853864\n",
      "POSSIBLE 17 6 377197.2634798437\n",
      "POSSIBLE 17 7 265709.34352940286\n",
      "POSSIBLE 17 8 422362.96513294015\n",
      "POSSIBLE 17 9 356741.1526939361\n",
      "POSSIBLE 17 10 339512.80585712724\n",
      "POSSIBLE 17 11 330854.3368667036\n",
      "POSSIBLE 17 12 300764.6084865363\n",
      "POSSIBLE 17 13 241447.8061904074\n",
      "POSSIBLE 17 14 263726.8857656127\n",
      "POSSIBLE 17 15 341795.98309464054\n",
      "POSSIBLE 17 16 350899.5777636245\n",
      "Cluster with spot size (100, False, 17) completed.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "resolutions = [0.1, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0]\n",
    "for spot_size in [50, 75, 100]:\n",
    "    for third_dim in [False]:\n",
    "        for K in [17]:\n",
    "            original_data, k_means_cluster, k_means_cluster_no_spatial = run_experiment(df_transcripts, \"hBreast\", spot_size, third_dim, resolutions, n_clusters=K)\n",
    "            # K-Means Spatial\n",
    "            if \"K-Means\" not in cluster_dict:\n",
    "                cluster_dict[\"K-Means\"] = {}\n",
    "            if spot_size not in cluster_dict[\"K-Means\"]:\n",
    "                cluster_dict[\"K-Means\"][spot_size] = {}\n",
    "            cluster_dict[\"K-Means\"][spot_size][third_dim] = {True: {K: k_means_cluster.tolist()}}\n",
    "            record_results(original_data, cluster_dict, results_dir, \"K-Means\", cluster_results_filename, spot_size, third_dim, K, uses_spatial=True)\n",
    "\n",
    "            # K-Means No Spatial\n",
    "            if \"K-Means_No_Spatial\" not in cluster_dict:\n",
    "                cluster_dict[\"K-Means_No_Spatial\"] = {}\n",
    "            if spot_size not in cluster_dict[\"K-Means_No_Spatial\"]:\n",
    "                cluster_dict[\"K-Means_No_Spatial\"][spot_size] = {}\n",
    "            cluster_dict[\"K-Means_No_Spatial\"][spot_size][third_dim] = {False: {K: k_means_cluster_no_spatial.tolist()}}\n",
    "            record_results(original_data, cluster_dict, results_dir, \"K-Means_No_Spatial\", cluster_results_filename, spot_size, third_dim, K, uses_spatial=False)\n",
    "\n",
    "            print(f\"Cluster with spot size {(spot_size, third_dim, K)} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "75\n",
      "100\n",
      "100\n",
      "Method: K-Means Spot Size 100 Num Clusters: 17 Total WSS 0.005876699744974874\n",
      "50\n",
      "75\n",
      "100\n",
      "100\n",
      "Method: K-Means_No_Spatial Spot Size 100 Num Clusters: 17 Total WSS 0.00595558771854054\n"
     ]
    }
   ],
   "source": [
    "spot_sizes = [50,75,100]\n",
    "resolutions = [0.25, 0.5, 0.75, 1.0]\n",
    "methods = [\"K-Means\", \"K-Means_No_Spatial\"]\n",
    "in_billions = 1_000_000_000\n",
    "for method in methods:\n",
    "    for spot_size in spot_sizes:\n",
    "        print(spot_size)\n",
    "        for K in [17]:\n",
    "            filename = f\"results/hBreast/{method}/{K}/wss/{spot_size}/{cluster_results_filename}_wss.json\"\n",
    "            if os.path.exists(filename):\n",
    "                print(spot_size)\n",
    "                with open(filename, \"r\") as wss_dict:\n",
    "                    current_wss = json.load(wss_dict)\n",
    "                print(\"Method:\", method, \"Spot Size\", spot_size, \"Num Clusters:\", len(current_wss), \"Total WSS\", sum(current_wss.values()) / in_billions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xenium-1YUjn3qu-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
