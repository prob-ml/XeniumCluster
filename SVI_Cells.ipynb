{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.distributions.constraints as constraints\n",
    "import pyro\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "import pyro.distributions as dist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import tensorboard\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from importlib import reload\n",
    "\n",
    "import torchclustermetrics \n",
    "reload(torchclustermetrics)\n",
    "from torchclustermetrics import silhouette\n",
    "\n",
    "# this ensures that I can update the class without losing my variables in my notebook\n",
    "import xenium_cluster\n",
    "reload(xenium_cluster)\n",
    "from xenium_cluster import XeniumCluster\n",
    "from utils.metrics import *\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your .gz file\n",
    "file_path = 'data/hBreast/transcripts.csv.gz'\n",
    "\n",
    "# Read the gzipped CSV file into a DataFrame\n",
    "df_transcripts = pd.read_csv(file_path, compression='gzip')\n",
    "df_transcripts[\"error_prob\"] = 10 ** (-df_transcripts[\"qv\"]/10)\n",
    "df_transcripts.head(), df_transcripts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cells without ids\n",
    "df_transcripts = df_transcripts[df_transcripts[\"cell_id\"] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = df_transcripts.groupby(['cell_id', 'feature_name']).size().reset_index(name='count')\n",
    "cells_pivot = cells.pivot_table(index='cell_id', \n",
    "                                columns='feature_name', \n",
    "                                values='count', \n",
    "                                fill_value=0)\n",
    "cells_pivot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_means = df_transcripts.groupby('cell_id').agg({\n",
    "    'x_location': 'mean',\n",
    "    'y_location': 'mean',\n",
    "    'z_location': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "cells_pivot = location_means.join(cells_pivot, on='cell_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log normalization\n",
    "cells_pivot.iloc[:, 4:] = np.log1p(cells_pivot.iloc[:, 4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca_data = pca.fit_transform(cells_pivot.iloc[:, 4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 6\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (5 PCs for each spot)\n",
    "data = torch.tensor(pca_data).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_loss(model, guide, data, original_positions, batch_size=256, weight=100.0, sigma=1.0, *args, **kwargs):\n",
    "\n",
    "    elbo_loss_fn = Trace_ELBO(num_particles=10).differentiable_loss\n",
    "    elbo_loss = elbo_loss_fn(model, guide, data, *args, **kwargs)\n",
    "    \n",
    "    def smoothness_loss(cluster_probs, original_positions, sigma):\n",
    "        pairwise_distances = torch.cdist(original_positions, original_positions, p=2)\n",
    "        adjacency_matrix = torch.exp(-pairwise_distances**2 / (2 * sigma**2))\n",
    "        # cluster_probs = F.softmax(cluster_probs, dim=1)\n",
    "        cluster_probs = F.gumbel_softmax(cluster_probs, tau=0.25, dim=1)\n",
    "        diffs = cluster_probs.unsqueeze(1) - cluster_probs.unsqueeze(0)\n",
    "        smoothness_loss_value = torch.sum(adjacency_matrix * torch.sum(diffs**2, dim=-1))\n",
    "        return smoothness_loss_value\n",
    "    \n",
    "    with pyro.plate(\"data\", len(original_positions), subsample_size=batch_size) as ind:\n",
    "        cluster_probs = (pyro.param(\"cluster_concentration_params_q\")[ind])\n",
    "        positions = original_positions[ind]\n",
    "    \n",
    "    spatial_loss_value = smoothness_loss(cluster_probs, positions, sigma)\n",
    "    \n",
    "    total_loss = elbo_loss + weight * spatial_loss_value\n",
    "    \n",
    "    # print(f\"ELBO: {elbo_loss.item()}, SPATIAL: {weight * spatial_loss_value.item()}, CUSTOM: {total_loss.item()}\")\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    # Define priors for the cluster assignment probabilities and Gaussian parameters\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=BATCH_SIZE) as ind:\n",
    "        batch_data = data[ind]\n",
    "        cluster_probs = pyro.sample(\"cluster_probs\", dist.Dirichlet(torch.ones(BATCH_SIZE, NUM_CLUSTERS)))\n",
    "        \n",
    "        # Define the means and variances of the Gaussian components\n",
    "        cluster_means = pyro.sample(\"cluster_means\", dist.Normal(0., 1.).expand([NUM_CLUSTERS, batch_data.size(1)]).to_event(2))\n",
    "        cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(0., 1.).expand([NUM_CLUSTERS, batch_data.size(1)]).to_event(2))\n",
    "        \n",
    "        # Likelihood of data given cluster assignments\n",
    "        pyro.sample(\"obs\", dist.MixtureOfDiagNormals(cluster_means, cluster_scales, cluster_probs).to_event(1), obs=batch_data)\n",
    "\n",
    "def guide(data):\n",
    "    # Initialize cluster assignment probabilities for the entire dataset\n",
    "    MIN_CONCENTRATION = 0.1\n",
    "    cluster_concentration_params_q = pyro.param(\"cluster_concentration_params_q\", torch.ones(data.size(0), NUM_CLUSTERS), constraint=dist.constraints.positive) + MIN_CONCENTRATION\n",
    "    # Global variational parameters for means and scales\n",
    "    cluster_means_q_mean = pyro.param(\"cluster_means_q\", torch.randn(NUM_CLUSTERS, data.size(1)))\n",
    "    cluster_scales_q_mean = pyro.param(\"cluster_scales_q\", torch.ones(NUM_CLUSTERS, data.size(1)), constraint=dist.constraints.positive)\n",
    "    \n",
    "    with pyro.plate(\"data\", len(data), subsample_size=BATCH_SIZE) as ind:\n",
    "\n",
    "        batch_cluster_concentration_params_q = cluster_concentration_params_q[ind]\n",
    "\n",
    "        # pyro.sample(\"cluster_assignments\", dist.Categorical(batch_cluster_probs_q))\n",
    "        pyro.sample(\"cluster_probs\", dist.Dirichlet(batch_cluster_concentration_params_q))\n",
    "        pyro.sample(\"cluster_means\", dist.Normal(cluster_means_q_mean, 0.1).to_event(2))\n",
    "        pyro.sample(\"cluster_scales\", dist.LogNormal(cluster_scales_q_mean, 0.1).to_event(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import PyroOptim, PyroLRScheduler\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "starting_lr = 0.01\n",
    "ending_lr = 0.00001\n",
    "N_STEPS = 100000\n",
    "\n",
    "# Setup the optimizer\n",
    "adam_params = {\"lr\": 0.01, \"betas\": (0.90, 0.999)}\n",
    "optimizer = PyroOptim(Adam, adam_params)\n",
    "scheduler = PyroLRScheduler(lr_scheduler.StepLR, {'optimizer': Adam, 'optim_args': {'lr': starting_lr}, 'step_size': 1, 'gamma': (ending_lr / starting_lr) ** (1 / N_STEPS)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_positions = torch.tensor(cells_pivot[[\"x_location\", \"y_location\"]].to_numpy())\n",
    "original_positions = (original_positions - original_positions.mean(dim=0)) / original_positions.std(dim=0)\n",
    "# Setup the inference algorithm\n",
    "svi = SVI(model, guide, scheduler, loss=lambda model, guide, data, original_positions: spatial_loss(model, guide, data, original_positions))\n",
    "\n",
    "# Setup the inference algorithm\n",
    "# svi = SVI(model, guide, scheduler, loss=Trace_ELBO(num_particles=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do gradient steps\n",
    "for step in range(N_STEPS):\n",
    "    loss = svi.step(data, original_positions)\n",
    "    # loss = svi.step(data)\n",
    "    svi.optim.step()\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} : loss = {round(loss/1e6, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the learned variational parameters\n",
    "cluster_concentration_params_q = pyro.param(\"cluster_concentration_params_q\")\n",
    "cluster_probs_q = pyro.sample(\"cluster_probs\", dist.Dirichlet(cluster_concentration_params_q))\n",
    "cluster_concentration_params_q = cluster_concentration_params_q.detach().numpy()\n",
    "\n",
    "cluster_assignments_q = cluster_probs_q.argmax(dim=1)\n",
    "cluster_means_q_mean = pyro.param(\"cluster_means_q\").detach().numpy()\n",
    "cluster_scales_q_mean = pyro.param(\"cluster_scales_q\").detach().numpy()\n",
    "\n",
    "# Output the learned cluster probabilities for each data point\n",
    "print(cluster_assignments_q, cluster_probs_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cluster_assignments_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True) \n",
    "np.round(cluster_means_q_mean, 4), np.round(cluster_scales_q_mean, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(cells_pivot[\"x_location\"], cells_pivot[\"y_location\"], s=1, c=cluster_assignments_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
