{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import pyro\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from pyro.infer import SVI, Trace_ELBO, TraceMeanField_ELBO\n",
    "from pyro.optim import PyroOptim\n",
    "from torch.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.cm import get_cmap\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial import KDTree\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from importlib import reload\n",
    "\n",
    "# this ensures that I can update the class without losing my variables in my notebook\n",
    "import xenium_cluster\n",
    "reload(xenium_cluster)\n",
    "from xenium_cluster import XeniumCluster\n",
    "from utils.metrics import *\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"YAY! GPU available :3\")\n",
    "    \n",
    "    # Get all available GPUs sorted by memory usage (lowest first)\n",
    "    available_gpus = GPUtil.getAvailable(order='memory', limit=1)\n",
    "    \n",
    "    if available_gpus:\n",
    "        selected_gpu = available_gpus[0]\n",
    "        \n",
    "        # Set the GPU with the lowest memory usage\n",
    "        torch.cuda.set_device(selected_gpu)\n",
    "        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        print(f\"Using GPU: {selected_gpu} with the lowest memory usage.\")\n",
    "    else:\n",
    "        print(\"No GPUs available with low memory usage.\")\n",
    "else:\n",
    "    print(\"No GPU available :(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_DLPFC_data(\n",
    "    section_id=151670,\n",
    "    num_pcs=5,\n",
    "    log_normalize=True,\n",
    "):\n",
    "    section = ad.read_h5ad(f\"data/DLPFC/{section_id}.h5ad\")\n",
    "    section.var[\"feature_name\"] = section.var.index\n",
    "\n",
    "    spatial_locations = section.obs[[\"array_row\", \"array_col\"]]\n",
    "    spatial_locations.columns = [\"row\", \"col\"]\n",
    "\n",
    "    clustering = XeniumCluster(data=section.X, dataset_name=\"DLPFC\")\n",
    "    clustering.xenium_spot_data = section\n",
    "    if log_normalize:\n",
    "        clustering.xenium_spot_data.X = np.log1p(clustering.xenium_spot_data.X)\n",
    "\n",
    "    clustering.pca(clustering.xenium_spot_data, num_pcs)\n",
    "    data = clustering.xenium_spot_data.obsm[\"X_pca\"]\n",
    "\n",
    "    return data, spatial_locations, clustering\n",
    "\n",
    "def prepare_Xenium_data(\n",
    "        dataset=\"hBreast\", \n",
    "        spots=True, \n",
    "        spot_size=100, \n",
    "        third_dim=False, \n",
    "        log_normalize=True,\n",
    "        likelihood_mode=\"PCA\",\n",
    "        num_pcs=5,\n",
    "        hvg_var_prop=0.5,\n",
    "        min_expressions_per_spot=10\n",
    "    ):\n",
    "\n",
    "    data_filepath = f\"data/spot_data/{dataset}/hBreast_SPOTSIZE={spot_size}um_z={third_dim}.h5ad\"\n",
    "    \n",
    "    if spots:\n",
    "\n",
    "        if os.path.exists(data_filepath):\n",
    "\n",
    "            clustering = XeniumCluster(data=None, dataset_name=\"hBreast\")\n",
    "            clustering.set_spot_size(spot_size)\n",
    "            print(\"Loading data.\")\n",
    "            clustering.xenium_spot_data = ad.read_h5ad(data_filepath)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Path to your .gz file\n",
    "            file_path = f'data/{dataset}/transcripts.csv.gz'\n",
    "\n",
    "            # Read the gzipped CSV file into a DataFrame\n",
    "            df_transcripts = pd.read_csv(file_path, compression='gzip')\n",
    "            df_transcripts[\"error_prob\"] = 10 ** (-df_transcripts[\"qv\"]/10)\n",
    "            df_transcripts.head(), df_transcripts.shape\n",
    "\n",
    "            # drop cells without ids\n",
    "            df_transcripts = df_transcripts[df_transcripts[\"cell_id\"] != -1]\n",
    "\n",
    "            # drop blanks and controls\n",
    "            df_transcripts = df_transcripts[~df_transcripts[\"feature_name\"].str.startswith('BLANK_') & ~df_transcripts[\"feature_name\"].str.startswith('NegControl')]\n",
    "\n",
    "            clustering = XeniumCluster(data=df_transcripts, dataset_name=\"hBreast\")\n",
    "            clustering.set_spot_size(spot_size)\n",
    "\n",
    "            if not os.path.exists(data_filepath):\n",
    "                print(\"Generating and saving data\")\n",
    "                clustering.create_spot_data(third_dim=third_dim, save_data=True)\n",
    "                clustering.xenium_spot_data.write_h5ad(data_filepath)\n",
    "\n",
    "        print(\"Number of spots: \", clustering.xenium_spot_data.shape[0])\n",
    "        clustering.xenium_spot_data = clustering.xenium_spot_data[clustering.xenium_spot_data.X.sum(axis=1) > min_expressions_per_spot]\n",
    "        print(\"Number of spots after filtering: \", clustering.xenium_spot_data.shape[0])\n",
    "\n",
    "        if log_normalize:\n",
    "            clustering.normalize_counts(clustering.xenium_spot_data)\n",
    "\n",
    "        if likelihood_mode == \"PCA\":\n",
    "            clustering.pca(clustering.xenium_spot_data, num_pcs)\n",
    "            data = clustering.xenium_spot_data.obsm[\"X_pca\"]\n",
    "        elif likelihood_mode == \"HVG\":\n",
    "            min_dispersion = torch.distributions.normal.Normal(0.0, 1.0).icdf(hvg_var_prop)\n",
    "            clustering.filter_only_high_variable_genes(clustering.xenium_spot_data, flavor=\"seurat\", min_mean=0.0125, max_mean=1000, min_disp=min_dispersion)\n",
    "            clustering.xenium_spot_data = clustering.xenium_spot_data[:,clustering.xenium_spot_data.var.highly_variable==True]\n",
    "            data = clustering.xenium_spot_data.X\n",
    "        elif likelihood_mode == \"ALL\":\n",
    "            data = clustering.xenium_spot_data.X\n",
    "\n",
    "        spatial_locations = clustering.xenium_spot_data.obs[[\"row\", \"col\"]]\n",
    "    \n",
    "    # prepare cells data\n",
    "    else:\n",
    "\n",
    "        cells = df_transcripts.groupby(['cell_id', 'feature_name']).size().reset_index(name='count')\n",
    "        cells_pivot = cells.pivot_table(index='cell_id', \n",
    "                                        columns='feature_name', \n",
    "                                        values='count', \n",
    "                                        fill_value=0)\n",
    "        \n",
    "        location_means = df_transcripts.groupby('cell_id').agg({\n",
    "            'x_location': 'mean',\n",
    "            'y_location': 'mean',\n",
    "            'z_location': 'mean'\n",
    "        }).reset_index()\n",
    "\n",
    "        cells_pivot = location_means.join(cells_pivot, on='cell_id')\n",
    "\n",
    "        if log_normalize:\n",
    "            # log normalization\n",
    "            cells_pivot.iloc[:, 4:] = np.log1p(cells_pivot.iloc[:, 4:])\n",
    "\n",
    "        if likelihood_mode == \"PCA\":\n",
    "            pca = PCA(n_components=num_pcs)\n",
    "            data = pca.fit_transform(cells_pivot.iloc[:, 4:])\n",
    "\n",
    "        elif likelihood_mode == \"HVG\":\n",
    "            genes = cells_pivot.iloc[:, 4:]\n",
    "            gene_variances = genes.var(axis=0)\n",
    "            gene_variances = gene_variances.sort_values(ascending=False)\n",
    "            gene_var_proportions = (gene_variances / sum(gene_variances))\n",
    "            relevant_genes = list(gene_var_proportions[(gene_var_proportions.cumsum() < hvg_var_prop)].index)\n",
    "            cells_pivot.iloc[:, 4:] = cells_pivot.iloc[:, 4:][[relevant_genes]]\n",
    "            data = cells_pivot.iloc[:, 4:]\n",
    "\n",
    "        elif likelihood_mode == \"ALL\":\n",
    "            data = cells_pivot.iloc[:, 4:]\n",
    "\n",
    "        spatial_locations = cells_pivot[[\"x_location\", \"y_location\"]]\n",
    "\n",
    "    # the last one is to regain var/obs access from original data\n",
    "    return data, spatial_locations, clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_size=50\n",
    "data_mode=\"PCA\"\n",
    "num_pcs=3\n",
    "hvg_var_prop=0.5\n",
    "dataset_name=\"hBreast\"\n",
    "custom_init=\"K-Means\"\n",
    "spatial_init=True\n",
    "num_clusters=17\n",
    "batch_size=512\n",
    "neighborhood_size=1\n",
    "neighborhood_agg=\"mean\"\n",
    "concentration_amplification=20.0\n",
    "uncertainty_values = [1/num_clusters, 2/num_clusters, 3/num_clusters, 4/num_clusters, 5/num_clusters]\n",
    "evaluate_markers=False\n",
    "spatial_normalize=0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data, spatial_locations, original_adata = prepare_Xenium_data(\n",
    "    dataset=\"hBreast\", \n",
    "    spots=True, \n",
    "    spot_size=spot_size, \n",
    "    third_dim=False, \n",
    "    log_normalize=True, \n",
    "    likelihood_mode=data_mode, \n",
    "    num_pcs=num_pcs,\n",
    "    hvg_var_prop=hvg_var_prop,\n",
    "    min_expressions_per_spot=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_filepath(model, component, sample_for_assignment=None):\n",
    "\n",
    "    total_file_path = (\n",
    "        f\"results/{dataset_name}/{model}/{component}/{data_file_path}/\"\n",
    "        f\"INIT={custom_init}/NEIGHBORSIZE={neighborhood_size}/NUMCLUSTERS={num_clusters}\"\n",
    "        f\"/SPATIALINIT={spatial_init}/SAMPLEFORASSIGNMENT={sample_for_assignment}\"\n",
    "        f\"/SPATIALNORM={spatial_normalize}/SPATIALPRIORMULT={concentration_amplification}/SPOTSIZE={spot_size}/AGG={neighborhood_agg}\"\n",
    "    )\n",
    "\n",
    "    return total_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cluster_initialization(original_adata, method, K=17):\n",
    "\n",
    "    original_adata.generate_neighborhood_graph(original_adata.xenium_spot_data, plot_pcas=False)\n",
    "\n",
    "    # This function initializes clusters based on the specified method\n",
    "    if method == \"K-Means\":\n",
    "        initial_clusters = original_adata.KMeans(original_adata.xenium_spot_data, save_plot=False, K=K, include_spatial=False)\n",
    "    elif method == \"Hierarchical\":\n",
    "        initial_clusters = original_adata.Hierarchical(original_adata.xenium_spot_data, save_plot=True, K=K)\n",
    "    elif method == \"Leiden\":\n",
    "        initial_clusters = original_adata.Leiden(original_adata.xenium_spot_data, resolutions=[0.75], save_plot=False, K=K)[0.75]\n",
    "    elif method == \"Louvain\":\n",
    "        initial_clusters = original_adata.Louvain(original_adata.xenium_spot_data, resolutions=[1.0], save_plot=False, K=K)[1.0]\n",
    "    elif method == \"mclust\":\n",
    "        original_adata.pca(original_adata.xenium_spot_data, num_pcs)\n",
    "        initial_clusters = original_adata.mclust(original_adata.xenium_spot_data, G=K, model_name = \"EEE\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    return initial_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamping\n",
    "MIN_CONCENTRATION = 0.001\n",
    "\n",
    "num_posterior_samples = 100\n",
    "\n",
    "spatial_init_data = StandardScaler().fit_transform(gene_data)\n",
    "gene_data = StandardScaler().fit_transform(gene_data)\n",
    "empirical_prior_means = torch.zeros(num_clusters, spatial_init_data.shape[1])\n",
    "empirical_prior_scales = torch.ones(num_clusters, spatial_init_data.shape[1])\n",
    "\n",
    "rows = spatial_locations[\"row\"].astype(int)\n",
    "columns = spatial_locations[\"col\"].astype(int)\n",
    "\n",
    "num_rows = max(rows) + 1\n",
    "num_cols = max(columns) + 1\n",
    "\n",
    "if custom_init:\n",
    "\n",
    "    initial_clusters = custom_cluster_initialization(original_adata, custom_init)\n",
    "\n",
    "\n",
    "    match data_mode:\n",
    "        case \"PCA\":\n",
    "            data_file_path = f\"{data_mode}/{num_pcs}\"\n",
    "        case \"HVG\": \n",
    "            data_file_path = f\"{data_mode}/{hvg_var_prop}\"\n",
    "        case \"ALL\":\n",
    "            data_file_path = f\"{data_mode}\"\n",
    "        case _:\n",
    "            raise ValueError(\"The data mode specified is not supported.\")\n",
    "\n",
    "    if dataset_name == \"DLPFC\":\n",
    "        # Create a DataFrame for easier handling\n",
    "        data = pd.DataFrame({\n",
    "            'ClusterAssignments': initial_clusters,\n",
    "            'Region': original_adata.xenium_spot_data.obs[\"Region\"]\n",
    "        })\n",
    "\n",
    "        # Drop rows where 'Region' is NaN\n",
    "        filtered_data = data.dropna(subset=['Region'])\n",
    "\n",
    "        # Calculate ARI and NMI only for the non-NaN entries\n",
    "        ari = ARI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "        nmi = NMI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "        cluster_metrics = {\n",
    "            \"ARI\": ari,\n",
    "            \"NMI\": nmi\n",
    "        }\n",
    "\n",
    "        data_file_path = f\"{data_mode}/{num_pcs}\"\n",
    "\n",
    "        if not os.path.exists(kmeans_cluster_metrics_filepath := save_filepath(\"KMeans\", \"cluster_metrics\")):\n",
    "            os.makedirs(kmeans_cluster_metrics_filepath)\n",
    "        with open(f\"{kmeans_cluster_metrics_filepath}/wss.json\", 'w') as fp:\n",
    "            json.dump(cluster_metrics, fp)\n",
    "\n",
    "    for i in range(num_clusters):\n",
    "        cluster_data = gene_data[initial_clusters == i]\n",
    "        if cluster_data.size > 0:  # Check if there are any elements in the cluster_data\n",
    "            empirical_prior_means[i] = torch.tensor(cluster_data.mean(axis=0))\n",
    "            empirical_prior_scales[i] = torch.tensor(cluster_data.std(axis=0))\n",
    "    concentration_priors = torch.zeros((initial_clusters.shape[0], num_clusters))\n",
    "    concentration_priors[torch.arange(initial_clusters.shape[0]), initial_clusters - 1] = 1.\n",
    "\n",
    "else:\n",
    "\n",
    "    concentration_priors = torch.ones((len(gene_data), num_clusters), dtype=float)\n",
    "\n",
    "locations_tensor = torch.tensor(spatial_locations.to_numpy())\n",
    "\n",
    "# Compute the number of elements in each dimension\n",
    "num_spots = concentration_priors.shape[0]\n",
    "\n",
    "# Initialize an empty tensor for spatial concentration priors\n",
    "spatial_concentration_priors = torch.zeros_like(concentration_priors, dtype=torch.float64)\n",
    "\n",
    "spot_locations = KDTree(locations_tensor.cpu())  # Ensure this tensor is in host memory\n",
    "neighboring_spot_indexes = spot_locations.query_ball_point(locations_tensor.cpu(), r=neighborhood_size, p=1, workers=8)\n",
    "\n",
    "# Iterate over each spot\n",
    "for i in tqdm(range(num_spots)):\n",
    "\n",
    "    # Select priors in the neighborhood\n",
    "    priors_in_neighborhood = concentration_priors[neighboring_spot_indexes[i]]\n",
    "    # print(f\"Spot {i} has {len(neighboring_spot_indexes[i])} neighbors\")\n",
    "    # print(priors_in_neighborhood)\n",
    "\n",
    "    # Compute the sum or mean, or apply a custom weighting function\n",
    "    if neighborhood_agg == \"sum\":\n",
    "        neighborhood_expression = priors_in_neighborhood.sum(dim=0)\n",
    "    elif neighborhood_agg == \"mean\":\n",
    "        neighborhood_expression = priors_in_neighborhood.mean(dim=0)\n",
    "    else:\n",
    "        locations = original_adata.xenium_spot_data.obs[[\"x_location\", \"y_location\", \"z_location\"]].values\n",
    "        neighboring_locations = locations[neighboring_spot_indexes[i]].astype(float)\n",
    "        distances = torch.tensor(np.linalg.norm(neighboring_locations - locations[i], axis=1))\n",
    "        def distance_weighting(x):\n",
    "            weight = 1/(1 + x/spot_size)\n",
    "            # print(weight)\n",
    "            return weight\n",
    "        neighborhood_expression = (priors_in_neighborhood * distance_weighting(distances).reshape(-1, 1)).sum(dim=0)\n",
    "    # Update spatial concentration priors with the computed neighborhood expression\n",
    "    # print(neighborhood_expression)\n",
    "    spatial_concentration_priors[i] += neighborhood_expression\n",
    "\n",
    "# Normalize concentration priors\n",
    "concentration_priors = torch.tensor(spatial_concentration_priors, dtype=torch.float64)\n",
    "concentration_priors /= concentration_priors.sum(dim=1, keepdim=True)\n",
    "concentration_priors *= num_clusters * concentration_amplification\n",
    "concentration_priors = concentration_priors.clamp(MIN_CONCENTRATION)\n",
    "\n",
    "sample_for_assignment_options = [True, False]\n",
    "\n",
    "for sample_for_assignment in sample_for_assignment_options:\n",
    "\n",
    "    if sample_for_assignment:\n",
    "        cluster_probs_prior_TRUE = pyro.sample(\"cluster_probs\", dist.Dirichlet(concentration_priors).expand_by([num_posterior_samples])).detach().mean(dim=0)\n",
    "        cluster_assignments_prior = pyro.sample(\"cluster_assignments\", dist.Categorical(cluster_probs_prior_TRUE)) \n",
    "    else:\n",
    "        # the probs aren't sampled and we calculate the EV instead\n",
    "        cluster_probs_prior_FALSE = (concentration_priors / concentration_priors.sum(dim=1, keepdim=True))\n",
    "        cluster_assignments_prior = cluster_probs_prior_FALSE.argmax(dim=1)\n",
    "\n",
    "    # Load the data\n",
    "    data = torch.tensor(gene_data).float()\n",
    "\n",
    "    cluster_grid_PRIOR = torch.zeros((num_rows, num_cols), dtype=torch.long)\n",
    "\n",
    "    cluster_grid_PRIOR[rows, columns] = cluster_assignments_prior + 1\n",
    "\n",
    "    colors = plt.cm.get_cmap('viridis', num_clusters + 1)\n",
    "    \n",
    "    colormap_colors = np.vstack(([[1, 1, 1, 1]], colors(np.linspace(0, 1, num_clusters))))\n",
    "    colormap = ListedColormap(colormap_colors)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cluster_grid_PRIOR.cpu(), cmap=colormap, interpolation='nearest', origin='lower')\n",
    "    plt.colorbar(ticks=range(num_clusters + 1), label='Cluster Values')\n",
    "    plt.title('Prior Cluster Assignment with BayXenSmooth')\n",
    "\n",
    "    if not os.path.exists(bayxensmooth_clusters_filepath := save_filepath(\"BayXenSmooth\", \"clusters\", sample_for_assignment)):\n",
    "        os.makedirs(bayxensmooth_clusters_filepath)\n",
    "    _ = plt.savefig(\n",
    "        f\"{bayxensmooth_clusters_filepath}/prior_result.png\"\n",
    "    )\n",
    "\n",
    "    # grab the WSS distance of cluster labels\n",
    "    wss = {}\n",
    "    for label in range(1, num_clusters + 1):\n",
    "        current_cluster_locations = torch.stack(torch.where((cluster_grid_PRIOR.cpu() == label)), axis=1).to(float)\n",
    "        wss[f\"Cluster {label}\"] = (spot_size ** 2) * torch.mean(torch.cdist(current_cluster_locations, current_cluster_locations, p = 2)).item()\n",
    "    print(sample_for_assignment, sum(wss.values()) / 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concentration_priors[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_prior_means_guide = empirical_prior_means.clone().detach()\n",
    "empirical_prior_scales_guide = empirical_prior_scales.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "PRIOR_SCALE = np.sqrt(0.1) # higher means weaker\n",
    "NUM_PARTICLES = 25\n",
    "\n",
    "expected_total_param_dim = 2 # K x D\n",
    "\n",
    "def model(data):\n",
    "\n",
    "    with pyro.plate(\"clusters\", num_clusters):\n",
    "\n",
    "        # Define the means and variances of the Gaussian components\n",
    "        cluster_means = pyro.sample(\"cluster_means\", dist.Normal(empirical_prior_means, 0.1).to_event(1))\n",
    "        cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(empirical_prior_scales, 0.1).to_event(1))\n",
    "\n",
    "    # Define priors for the cluster assignment probabilities and Gaussian parameters\n",
    "    with pyro.plate(\"data\", len(data), subsample_size=batch_size) as ind:\n",
    "        batch_data = data[ind]\n",
    "        batch_concentration_priors = concentration_priors[ind]\n",
    "        cluster_probs = pyro.sample(\"cluster_probs\", dist.Dirichlet(batch_concentration_priors))\n",
    "        # likelihood for batch\n",
    "        if cluster_means.dim() == expected_total_param_dim:\n",
    "            pyro.sample(f\"obs\", dist.MixtureOfDiagNormals(\n",
    "                    cluster_means.unsqueeze(0).expand(batch_size, -1, -1), \n",
    "                    cluster_scales.unsqueeze(0).expand(batch_size, -1, -1), \n",
    "                    cluster_probs\n",
    "                ), \n",
    "                obs=batch_data\n",
    "            )\n",
    "        # likelihood for batch WITH vectorization of particles\n",
    "        else:\n",
    "            pyro.sample(f\"obs\", dist.MixtureOfDiagNormals(\n",
    "                    cluster_means.unsqueeze(1).expand(-1, batch_size, -1, -1), \n",
    "                    cluster_scales.unsqueeze(1).expand(-1, batch_size, -1, -1), \n",
    "                    cluster_probs\n",
    "                ), \n",
    "                obs=batch_data\n",
    "            )\n",
    "\n",
    "def guide(data):\n",
    "    # Initialize cluster assignment probabilities for the entire dataset\n",
    "    cluster_concentration_params_q = pyro.param(\"cluster_concentration_params_q\", concentration_priors + torch.abs(torch.randn_like(concentration_priors)), constraint=dist.constraints.positive).clamp(min=MIN_CONCENTRATION)\n",
    "    \n",
    "    with pyro.plate(\"clusters\", num_clusters):\n",
    "        # Global variational parameters for means and scales\n",
    "        cluster_means_q = pyro.param(\"cluster_means_q\", empirical_prior_means + torch.randn_like(empirical_prior_means) * 0.05)\n",
    "        cluster_scales_q = pyro.param(\"cluster_scales_q\", empirical_prior_scales + torch.randn_like(empirical_prior_scales) * 0.01, constraint=dist.constraints.positive)     \n",
    "        cluster_means = pyro.sample(\"cluster_means\", dist.Normal(cluster_means_q, 0.5).to_event(1))\n",
    "        cluster_scales = pyro.sample(\"cluster_scales\", dist.LogNormal(cluster_scales_q, 0.25).to_event(1))\n",
    "    \n",
    "    with pyro.plate(\"data\", len(data), subsample_size=batch_size) as ind:\n",
    "\n",
    "        batch_cluster_concentration_params_q = cluster_concentration_params_q[ind].clamp(min=MIN_CONCENTRATION)\n",
    "        cluster_probs = pyro.sample(\"cluster_probs\", dist.Dirichlet(batch_cluster_concentration_params_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pyro.render_model(model, model_args=(data,), render_distributions=True, render_params=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.round(empirical_prior_means, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "NUM_EPOCHS = 40\n",
    "NUM_BATCHES = int(math.ceil(data.shape[0] / batch_size))\n",
    "\n",
    "# Setup the optimizer\n",
    "adam_params = {\"lr\": 0.001, \"betas\": (0.90, 0.999)}\n",
    "scheduler = PyroOptim(Adam, adam_params)\n",
    "\n",
    "# Setup the inference algorithm\n",
    "svi = SVI(model, guide, scheduler, loss=TraceMeanField_ELBO(num_particles=NUM_PARTICLES, vectorize_particles=True))\n",
    "\n",
    "# Create a DataLoader for the data\n",
    "# Convert data to CUDA tensors before creating the DataLoader\n",
    "data = data.to('cuda')\n",
    "\n",
    "def spatial_penalty(cluster_probs, neighbors):\n",
    "    penalty = 0\n",
    "    for spot, neighbor_spots in enumerate(neighbors):\n",
    "        for neighbor in neighbor_spots:\n",
    "            if cluster_probs[spot] != cluster_probs[neighbor]:\n",
    "                penalty += torch.sum((cluster_probs[spot] - cluster_probs[neighbor]) ** 2)\n",
    "    return penalty\n",
    "\n",
    "# Clear the param store in case we're in a REPL\n",
    "pyro.clear_param_store()\n",
    "\n",
    "epoch_pbar = tqdm(range(NUM_EPOCHS))\n",
    "cluster_means_trace = []\n",
    "cluster_scales_trace = []\n",
    "current_min_loss = float('inf')\n",
    "PATIENCE = 10\n",
    "patience_counter = 0\n",
    "for epoch in epoch_pbar:\n",
    "    epoch_pbar.set_description(f\"Epoch {epoch}\")\n",
    "    running_loss = 0.0\n",
    "    for step in range(NUM_BATCHES):\n",
    "        loss = svi.step(data)\n",
    "        running_loss += loss / batch_size\n",
    "        # running_loss += (loss + SPATIAL_PENALTY_WEIGHT * spatial_penalty()) / batch_size\n",
    "    # svi.optim.step()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch} : loss = {round(running_loss, 4)}\")\n",
    "        current_cluster_means = pyro.param(\"cluster_means_q\").detach().cpu().numpy()\n",
    "        cluster_means_trace.append(current_cluster_means)\n",
    "        current_cluster_scales = pyro.param(\"cluster_scales_q\").detach().cpu().numpy()\n",
    "        cluster_scales_trace.append(current_cluster_scales)\n",
    "        if running_loss > current_min_loss:\n",
    "            patience_counter += 1\n",
    "        else:\n",
    "            current_min_loss = running_loss\n",
    "            patience_counter = 0\n",
    "        if patience_counter >= PATIENCE:\n",
    "            break\n",
    "        test = pyro.param(\"cluster_concentration_params_q\").detach().cpu()[88]\n",
    "        # cluster_concentration_params_q = pyro.param(\"cluster_concentration_params_q\", concentration_priors, constraint=dist.constraints.positive).clamp(min=MIN_CONCENTRATION)\n",
    "        # if sample_for_assignment:\n",
    "        #     cluster_probs_q = pyro.sample(\"cluster_probs\", dist.Dirichlet(cluster_concentration_params_q, validate_args=True)).detach()     \n",
    "        # else:\n",
    "        #     # the probs aren't sampled and we calculate the EV instead\n",
    "        #     cluster_probs_q = (cluster_concentration_params_q / cluster_concentration_params_q.sum(dim=1, keepdim=True))\n",
    "        # cluster_assignments_q = cluster_probs_q.argmax(dim=1)\n",
    "\n",
    "        # if dataset_name == \"DLPFC\":\n",
    "        #     # Create a DataFrame for easier handling\n",
    "        #     cluster_data = pd.DataFrame({\n",
    "        #         'ClusterAssignments': cluster_assignments_q,\n",
    "        #         'Region': original_adata.xenium_spot_data.obs[\"Region\"]\n",
    "        #     })\n",
    "\n",
    "        #     # Drop rows where 'Region' is NaN\n",
    "        #     filtered_data = cluster_data.dropna(subset=['Region'])\n",
    "\n",
    "        #     # Calculate ARI and NMI only for the non-NaN entries\n",
    "        #     ari = ARI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "        #     nmi = NMI(filtered_data['ClusterAssignments'], filtered_data['Region'])\n",
    "        #     print(f\"Step {step} : ARI = {ari} NMI = {nmi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_prior_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param(\"cluster_means_q\").detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(pyro.param(\"cluster_means_q\").detach() - empirical_prior_means).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.param(\"cluster_concentration_params_q\").detach(), concentration_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pyro.param(\"cluster_concentration_params_q\").detach().argmax(dim=1) == concentration_priors.argmax(dim=1)).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_means_trace = np.array(cluster_means_trace)  # Shape: [num_epochs, num_clusters, data_dim]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs, num_clusters, data_dim = cluster_means_trace.shape\n",
    "\n",
    "for cluster_idx in range(num_clusters):\n",
    "    plt.figure()\n",
    "    for dim_idx in range(data_dim):\n",
    "        plt.plot(\n",
    "            range(num_epochs),\n",
    "            cluster_means_trace[:, cluster_idx, dim_idx],\n",
    "            label=f\"Dim {dim_idx}\"\n",
    "        )\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Cluster Mean Value\")\n",
    "    plt.title(f\"Trace Plot for Cluster {cluster_idx}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_scales_trace = np.array(cluster_scales_trace)  # Shape: [num_epochs, num_clusters, data_dim]\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# num_epochs, num_clusters, data_dim = cluster_scales_trace.shape\n",
    "\n",
    "# for cluster_idx in range(num_clusters):\n",
    "#     for dim_idx in range(data_dim):\n",
    "#         plt.plot(\n",
    "#             range(num_epochs),\n",
    "#             cluster_scales_trace[:, cluster_idx, dim_idx],\n",
    "#             label=f\"Cluster {cluster_idx}, Dim {dim_idx}\"\n",
    "#         )\n",
    "#         plt.xlabel(\"Epoch\")\n",
    "#         plt.ylabel(\"Cluster Scale Value\")\n",
    "#         plt.title(f\"Trace Plot for Cluster {cluster_idx}, Dimension {dim_idx}\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "# Grab the learned variational parameters\n",
    "num_posterior_samples = 1000\n",
    "sample_for_assignment_options = [True, False]\n",
    "\n",
    "for sample_for_assignment in sample_for_assignment_options:\n",
    "\n",
    "    cluster_concentration_params_q = pyro.param(\"cluster_concentration_params_q\")\n",
    "    if sample_for_assignment:\n",
    "        cluster_probs_q = pyro.sample(\"cluster_probs\", dist.Dirichlet(cluster_concentration_params_q).expand_by([num_posterior_samples])).detach().mean(dim=0)\n",
    "        # retrieve the relevant prior for comparison\n",
    "        cluster_assignments_prior = pyro.sample(\"cluster_assignments\", dist.Categorical(cluster_probs_prior_TRUE))\n",
    "        cluster_assignments_q = pyro.sample(\"cluster_assignments\", dist.Categorical(cluster_probs_q))\n",
    "    else:\n",
    "        # the probs aren't sampled and we calculate the EV instead\n",
    "        cluster_probs_q = (cluster_concentration_params_q / cluster_concentration_params_q.sum(dim=1, keepdim=True))\n",
    "        # retrieve the relevant prior for comparison\n",
    "        cluster_assignments_prior = cluster_probs_prior_FALSE.argmax(dim=1)\n",
    "        cluster_assignments_q = cluster_probs_q.argmax(dim=1)\n",
    "\n",
    "    \n",
    "    cluster_concentration_params_q = cluster_concentration_params_q.cpu().detach()\n",
    "    cluster_means_q = pyro.param(\"cluster_means_q\").cpu().detach()\n",
    "    cluster_scales_q = pyro.param(\"cluster_scales_q\").cpu().detach()\n",
    "    cluster_probs_q = cluster_probs_q.cpu().detach()\n",
    "    cluster_assignments_q = cluster_assignments_q.cpu().detach()\n",
    "    cluster_assignments_prior = cluster_assignments_prior.cpu().detach()\n",
    "\n",
    "    # Plotting\n",
    "    if spot_size:\n",
    "\n",
    "        rows = spatial_locations[\"row\"].astype(int)\n",
    "        columns = spatial_locations[\"col\"].astype(int)\n",
    "\n",
    "        num_rows = max(rows) + 1\n",
    "        num_cols = max(columns) + 1\n",
    "\n",
    "        cluster_grid = torch.zeros((num_rows, num_cols), dtype=torch.long)\n",
    "\n",
    "        cluster_grid[rows, columns] = cluster_assignments_q + 1\n",
    "\n",
    "        colors = plt.cm.get_cmap('viridis', num_clusters + 1)\n",
    "\n",
    "        colormap_colors = np.vstack(([[1, 1, 1, 1]], colors(np.linspace(0, 1, num_clusters))))\n",
    "        colormap = ListedColormap(colormap_colors)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(cluster_grid.cpu(), cmap=colormap, interpolation='nearest', origin='lower')\n",
    "        plt.colorbar(ticks=range(num_clusters + 1), label='Cluster Values')\n",
    "        plt.title('Posterior Cluster Assignment with BayXenSmooth')\n",
    "\n",
    "        match data_mode:\n",
    "            case \"PCA\":\n",
    "                data_file_path = f\"{data_mode}/{num_pcs}\"\n",
    "            case \"HVG\": \n",
    "                data_file_path = f\"{data_mode}/{hvg_var_prop}\"\n",
    "            case \"ALL\":\n",
    "                data_file_path = f\"{data_mode}\"\n",
    "            case _:\n",
    "                raise ValueError(\"The data mode specified is not supported.\")\n",
    "\n",
    "        if not os.path.exists(bayxensmooth_clusters_filepath := save_filepath(\"BayXenSmooth\", \"clusters\", sample_for_assignment)):\n",
    "            os.makedirs(bayxensmooth_clusters_filepath)\n",
    "        _ = plt.savefig(\n",
    "            f\"{bayxensmooth_clusters_filepath}/result.png\"\n",
    "        )\n",
    "\n",
    "        clusters = pd.DataFrame(cluster_assignments_q.cpu(), columns=[\"BayXenSmooth cluster\"]).to_csv(f\"{bayxensmooth_clusters_filepath}/clusters_K={num_clusters}.csv\")\n",
    "        soft_clusters = pd.DataFrame(cluster_probs_q, columns=[f'P(z_i = {i})'  for i in range(1, num_clusters + 1)]).to_csv(f\"{bayxensmooth_clusters_filepath}/soft_clusters_K={num_clusters}.csv\")\n",
    "\n",
    "        if not os.path.exists(bayxensmooth_similar_filepath := save_filepath(\"BayXenSmooth\", \"prior_v_posterior\", sample_for_assignment)):\n",
    "            os.makedirs(bayxensmooth_similar_filepath)\n",
    "        with open(f\"{bayxensmooth_similar_filepath}/similarity.txt\", 'w') as fp:\n",
    "            fp.write(str(torch.mean((cluster_assignments_prior == cluster_assignments_q).float()).item()))\n",
    "\n",
    "        # grab the WSS distance of cluster labels\n",
    "        wss = {}\n",
    "        for label in range(1, num_clusters + 1):\n",
    "            current_cluster_locations = torch.stack(torch.where((cluster_grid.cpu() == label)), axis=1).to(float)\n",
    "            wss[f\"Cluster {label}\"] = (spot_size ** 2) * torch.mean(torch.cdist(current_cluster_locations, current_cluster_locations, p = 2)).item()\n",
    "        print(\"WSS\", sum(wss.values()) / 1_000_000)\n",
    "\n",
    "        if not os.path.exists(bayxensmooth_wss_filepath := save_filepath(\"BayXenSmooth\", \"wss\", sample_for_assignment)):\n",
    "            os.makedirs(bayxensmooth_wss_filepath)\n",
    "        with open(f\"{bayxensmooth_wss_filepath}/wss.json\", 'w') as fp:\n",
    "            json.dump(wss, fp)\n",
    "\n",
    "        cmap = get_cmap('rainbow')\n",
    "\n",
    "        if isinstance(original_adata.xenium_spot_data.X, csr_matrix):\n",
    "            labels = np.unique(cluster_assignments_q)  # Define the number of clusters\n",
    "            gene_columns = original_adata.xenium_spot_data.var.index  # Column names from another source\n",
    "            mean_expression_by_cluster = pd.DataFrame(columns=gene_columns)\n",
    "\n",
    "            # Loop through each cluster label\n",
    "            for label in labels:\n",
    "                # Find indexes of current cluster\n",
    "                current_cluster_indexes = torch.where(cluster_assignments_q == label)[0].numpy()\n",
    "                \n",
    "                # Efficiently extract the rows for the current cluster using fancy indexing\n",
    "                expressions = original_adata.xenium_spot_data.X[current_cluster_indexes, :]\n",
    "                print(label, expressions.shape)\n",
    "                \n",
    "                # Compute mean expressions; the result is still a csr_matrix\n",
    "                mean_expressions = expressions.mean(axis=0)\n",
    "                \n",
    "                # Convert mean_expressions to a dense format and then to a DataFrame\n",
    "                mean_expressions_df = pd.DataFrame(mean_expressions.A, columns=gene_columns)\n",
    "                \n",
    "                # Append the result to the mean_expression_by_cluster DataFrame\n",
    "                mean_expression_by_cluster = pd.concat([mean_expression_by_cluster, mean_expressions_df], ignore_index=True)\n",
    "        else:\n",
    "            # identify marker genes within each cluster\n",
    "            mean_expression_by_cluster = pd.DataFrame(columns=original_adata.xenium_spot_data.var.index)\n",
    "\n",
    "            for label in range(num_clusters):\n",
    "                current_cluster_indexes = list(torch.where(cluster_assignments_q == label)[0].cpu().numpy())\n",
    "                expressions = pd.DataFrame(original_adata.xenium_spot_data.X, columns=original_adata.xenium_spot_data.var.index).iloc[current_cluster_indexes, :]\n",
    "                mean_expressions = expressions.mean(axis=0).to_frame().T\n",
    "                mean_expression_by_cluster = pd.concat([mean_expression_by_cluster, mean_expressions], ignore_index=True)\n",
    "\n",
    "        if evaluate_markers:\n",
    "            for i, gene in enumerate(mean_expression_by_cluster.columns):\n",
    "                # using subplots() to draw vertical lines \n",
    "                fig, ax = plt.subplots(figsize=(6, 6)) \n",
    "                ax.vlines(mean_expression_by_cluster[gene].index, ymin=0, ymax=mean_expression_by_cluster[gene], color=cmap(i / (len(mean_expression_by_cluster.columns) - 1))) \n",
    "                \n",
    "                # drawing the markers\n",
    "                ax.plot(mean_expression_by_cluster[gene].index, mean_expression_by_cluster[gene], \"^\", c=cmap(i / (len(mean_expression_by_cluster.columns) - 1))) \n",
    "                ax.set_ylim(0) \n",
    "                \n",
    "                # formatting and details \n",
    "                ax.set_xlabel('Cluster Label') \n",
    "                ax.set_ylabel('Mean Expression') \n",
    "                ax.set_title(gene) \n",
    "                ax.set_xticks(mean_expression_by_cluster[gene].index) \n",
    "                if not os.path.exists(bayxensmooth_expression_filepath := save_filepath(\"BayXenSmooth\", \"expressions\", sample_for_assignment)):\n",
    "                    os.makedirs(f\"{bayxensmooth_expression_filepath}\")\n",
    "                _ = plt.savefig(\n",
    "                    f\"{bayxensmooth_expression_filepath}/GENE={gene}.png\"\n",
    "                )\n",
    "        \n",
    "        # confidence mapping\n",
    "        cluster_confidences = torch.zeros((num_rows, num_cols), dtype=torch.double)\n",
    "\n",
    "        cluster_confidences[rows, columns] = cluster_probs_q.max(dim=1).values\n",
    "\n",
    "        colors = plt.cm.get_cmap('Greys', num_clusters + 1)\n",
    "        colormap = ListedColormap(colors(np.linspace(0, 1, num_clusters + 1)))\n",
    "\n",
    "        confidence_proportions = {}\n",
    "        for uncertainty_value in uncertainty_values:\n",
    "            confidence_matrix = (cluster_confidences > uncertainty_value).float()\n",
    "            confidence_proportions[uncertainty_value] = torch.mean(confidence_matrix).item()\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(cluster_confidences > uncertainty_value, cmap=colormap, interpolation='nearest', origin='lower')\n",
    "            plt.colorbar(ticks=range(num_clusters + 1), label='Cluster Values')\n",
    "            # PLOT ALL UNCERTAINTY VALUESs\n",
    "            plt.title(r'$P(z_i = k) > $' + f'{uncertainty_value}')\n",
    "            if not os.path.exists(bayxensmooth_uncertainty_filepath := save_filepath(\"BayXenSmooth\", \"uncertainty\", sample_for_assignment)):\n",
    "                os.makedirs(bayxensmooth_uncertainty_filepath)\n",
    "            _ = plt.savefig(\n",
    "                f\"{bayxensmooth_uncertainty_filepath}/CONFIDENCE={uncertainty_value}.png\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "\n",
    "        plt.scatter(spatial_locations[\"x_location\"], spatial_locations[\"y_location\"], s=1, c=cluster_assignments_q)\n",
    "        if not os.path.exists(bayxensmooth_clusters_filepath := save_filepath(\"BayXenSmooth\", \"clusters\", sample_for_assignment)):\n",
    "            os.makedirs(bayxensmooth_clusters_filepath)\n",
    "        _ = plt.savefig(\n",
    "            f\"{bayxensmooth_clusters_filepath}/result.png\"\n",
    "        )\n",
    "    \n",
    "    gene_data, spatial_locations, original_adata = prepare_Xenium_data(\n",
    "        dataset=\"hBreast\", \n",
    "        spots=True, \n",
    "        spot_size=spot_size, \n",
    "        third_dim=False, \n",
    "        log_normalize=False, \n",
    "        likelihood_mode=data_mode, \n",
    "        num_pcs=num_pcs,\n",
    "        hvg_var_prop=hvg_var_prop,\n",
    "        min_expressions_per_spot=0\n",
    "    )\n",
    "\n",
    "    print(\"BXS MORAN\", sample_for_assignment)\n",
    "    clusters = pd.DataFrame(cluster_assignments_q.cpu(), columns=[\"BayXenSmooth cluster\"])\n",
    "    morans_i_gene_dict = gene_morans_i(original_adata, spatial_locations, clusters[\"BayXenSmooth cluster\"])\n",
    "    # gearys_c_gene_dict = gene_gearys_c(original_adata, spatial_locations, clusters[\"BayXenSmooth cluster\"])\n",
    "    marker_genes = [\"BANK1\", \"CEACAM6\", \"FASN\", \"FGL2\", \"IL7R\", \"KRT6B\", \"POSTN\", \"TCIM\"]\n",
    "    morans_i_markers = {k: v for k, v in morans_i_gene_dict.items() if k in marker_genes}\n",
    "    # gearys_c_markers = {k: v for k, v in gearys_c_gene_dict.items() if k in marker_genes}\n",
    "    print(morans_i_markers)\n",
    "\n",
    "    print(\"INITIAL MORAN\", sample_for_assignment)\n",
    "    clusters = pd.DataFrame(initial_clusters, columns=[\"initial cluster\"])\n",
    "    morans_i_gene_dict = gene_morans_i(original_adata, spatial_locations, clusters[\"initial cluster\"])\n",
    "    # gearys_c_gene_dict = gene_gearys_c(original_adata, spatial_locations, clusters[\"BayXenSmooth cluster\"])\n",
    "    marker_genes = [\"BANK1\", \"CEACAM6\", \"FASN\", \"FGL2\", \"IL7R\", \"KRT6B\", \"POSTN\", \"TCIM\"]\n",
    "    morans_i_markers = {k: v for k, v in morans_i_gene_dict.items() if k in marker_genes}\n",
    "    # gearys_c_markers = {k: v for k, v in gearys_c_gene_dict.items() if k in marker_genes}\n",
    "    print(morans_i_markers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "same_labels = cluster_grid_PRIOR.cpu() == cluster_grid.cpu()\n",
    "plt.imshow(same_labels, interpolation='nearest', origin='lower')\n",
    "plt.colorbar(ticks=range(num_clusters + 1), label='Cluster Values')\n",
    "print(f\"Proportion of Same Labels: {same_labels.float().mean()}\")\n",
    "_ = plt.title('Initial Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xenium-1YUjn3qu-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
